---
layout:     post                    # 使用的布局(不需要改)
title:      NNDL&机器学习笔记             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-10-07              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

NNDL《神经网络与深度学习》及西瓜书《机器学习》笔记  

[B站网课链接](https://www.bilibili.com/video/BV13b4y1177W?p=1)

[书、课件等](https://nndl.github.io/)

## NNDL&机器学习笔记

* **NNDL《神经网络与深度学习》**

  * **学生提问**

    * ==为什么要使用logistic函数进行分类？==（第三章提问）
    * ==为什么正则化不约束bias？==（第四章提问）

    

  * **思考题**

    * $\mathrm{softmax}$回归解二分类问题相当于没有偏置的$\mathrm{logistic}$回归（第三章思考题）

    
    $$
    y_1=\mathbf w_1^\mathrm T\mathbf x\\
    y_2=\mathbf w_2^\mathrm T\mathbf x\\
    p_1=\frac{\exp(y1)}{\exp(y1)+\exp(y2)}=\frac{1}{1+\exp((\mathbf w_2^\mathrm T-\mathbf w_1^\mathrm T)\mathbf x)}\\
    p_2=1-p_1
    $$
    
  * **学习笔记**

  * 交叉熵及KL散度(Kullback-Leibler Divergence)（第三章）

    * ==**TODO 信息论看书 TODO**==

    * 自信息（Self Information）：一个随机事件所包含的信息量 $I(x)=-\log p(x)$

    * 熵：随机变量$X$自信息的数学期望，用来衡量一个随机事件的不确定性，对于分布$p(x)$

      
      $$
      H(X)=-\sum_{x\in \mathcal X}p(x)\log p(x)
      $$
      
    * 熵编码：在对分布$p(y)$的符号进行编码时，熵$H(p)$是**理论上最优的平均编码长度**，这种编码方式称为熵编码
    
      **理解**
    
      
    
    * KL散度定义：用概率分布$q$来近似$p$时所造成的信息损失量
    

    $$
    \begin{align}
    \mathrm {KL}(p,q)&=H(p,q)-H(p)\\
    &=\sum_xp(x)\log \frac{p(x)}{q(x)}
    \end{align}
    $$
    
  * 通用近似定理的理解（第四章）

    * [机器之心-在理解通用近似定理之前，你可能都不会理解神经网络](https://www.jiqizhixin.com/articles/2021-09-07-6)
    * 如果使用分段函数显然可以以任意精度近似有界闭集函数，使用类似的思想，考虑每个神经元像分段函数分段一样去拟合，于是对于单个神经元，要求输入$\mathbf x\in[\mathbf x_l,\mathbf x_r]$的时候输出$y=y_0$，而输入$\mathbf x\notin[\mathbf x_l,\mathbf x_r]$的时候输出$y=0$，实际上也就是输入在一个区间时神经元激活，不在时被抑制，即找到超平面把输入区间划分开来
    * 上面的输入区间不好划分，可以做个差分，也就是对于单个神经元，要求$\mathbf x\in[\mathbf x_l,\infin)$时$y=\Delta y$，$\mathbf x\in(-\infin,\mathbf x_l]$时$y=0$，这样就可以通过一个高斜率的超平面将区间划分开来了
    * 基于这样的通用近似定理，其实可以发现神经网络的近似能力并没有表现出智能，如果真的按照这样的方式去进行近似，虽然能够有很好的近似效果，但是不具备任何泛化能力，且需要的神经元数量是非常庞大的，几乎不可能在实际中使用。

  

* **西瓜书《机器学习》**

  * 第六章 支持向量机

    * 任意样本点$\mathbf x$到超平面$(\mathbf w,b)$的距离可写为
      
      
      $$
      |r|=\frac{|\mathbf w^\mathrm T\mathbf x+b|}{||\mathbf w||}
      $$
      
      
      **证明：**
      
      超平面方程：

      
      $$
      \mathbf w ^\mathrm T\mathbf x+b=0
      $$
      
      
      显然向量$\mathbf w$与该超平面垂直（向量$\mathbf t$与超平面平行充要条件为$\mathbf w^\mathrm T\mathbf t=0$）
      
      则有
      
      
      $$
      \mathbf w^\mathrm T(\mathbf x+r\frac{\mathbf w}{||\mathbf w||})+b=0\\
      r||\mathbf w||=-(\mathbf w^\mathrm T\mathbf x+b)
      $$
      
      
      
    * 支持向量机中的**核技巧应用**不是直接将样本从样本空间转换到特征空间，因为特征空间的维度可能很高，甚至是无穷维，为了避开这个障碍，不是直接进行转换，而是设置一个核函数表示两个样本在特征空间中的距离（不关注如何转换到特征空间）
      
      
      $$
      \kappa(\mathbf x_i,\mathbf x_j)=<\phi(\mathbf x_i),\phi(\mathbf x_j)>=\phi(\mathbf x_i)^\mathrm T\phi(\mathbf x_j)
      $$
      
      
      如高斯核：
    
      
      $$
      \kappa(\mathbf x_i,\mathbf x_j)=\exp(-\frac{||\mathbf x_i-\mathbf x_j||^2}{2\sigma^2})
      $$
      
    * 

