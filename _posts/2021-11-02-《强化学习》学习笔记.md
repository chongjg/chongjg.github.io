---
layout:     post                    # 使用的布局(不需要改)
title:      《强化学习》学习笔记             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-11-02              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

## 《强化学习》学习笔记

#### Chap1. 导论

* 强化学习的基本思想
  * 在智能体为了实现目标而不断与环境产生交互的过程中，抓住智能体所面对的真实问题的主要方面。
  * 具备学习能力的智能体必须能够在某种程度上感知环境的状态，然后采取动作影响环境状态。
  * 智能体必须同时拥有和环境状态相关的一个或多个明确的目标。
* 马尔科夫决策过程包含这三个方面
  * 感知、动作、目标

* 强化学习要素
  * 策略
    * 定义了学习智能体在特定时间的行为方式，环境状态到动作的映射
  * 收益信号
    * 定义了强化学习问题中的目标，表明短时间内什么是好的（基本由环境直接给予）
  * 价值函数
    * 表明了从长远的角度看什么是好的，可理解为对将来收益累加起来的期望（需要综合评估，难以确定）
  * 对环境建立的模型
    * 给定状态和动作，模型预测外部环境的下一个状态和收益（环境模型被用于做规划）
* Marvm Minsky 在他的博士论文中（Minsky, 1954）讨论了强化学习的计算方法，描述了他组装的一 台基于模拟信号的机器，他称其为“随机神经模拟强化计算器“, SNARCs(Stochastic Neural-Analog Reinforcement Calculators)，**模拟可修改的大脑突触连接**（第15章）

#### Chap2. 多臂赌博机

* $k$臂赌博机问题

  * 学习问题：

    * 重复地在$k$个选项或动作中进行选择。每一个时刻，选择某一个动作（臂），得到一定数值收益，收益仅由选择的动作对应的平稳概率分布决定。目标是某一段时间内最大化总收益的期望

  * 动作-价值函数

    * 
      $$
      q_*(a)=\mathbb E[R_t\vert A_t=a]\\
      Q_t(a)=\frac{t时刻前执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1}R_i\cdot \mathbb 1_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb 1_{A_i=a}}
      $$

    * 贪心

      * $A_t=\underset{a}{\arg\max}\ Q_t(a)$

    * 增量式更新

      * $$
        \begin{align}
        Q_{t+1}&=\frac{1}{t}\sum_{i=1}^tR_i\\
        &=Q_t+\frac{1}{t}[R_t-Q_t]
        \end{align}
        $$

* 基于置信度上界的动作选择

  * $\epsilon-$贪心会尝试选择非贪心动作，但是是盲目的。在非贪心动作中，最好是根据他们的潜力来选择事实上是最优的动作，这要考虑动作对应的估计价值以及这些估计的不确定性（或方差）
    * $A_t=\underset{a}{\arg\max}\Big[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}\Big]$
    * $N_t(a)$表示时刻$t$之前动作$a$被选择的次数，$c$控制试探的程度

* 梯度赌博机算法

  * 偏好函数$H_t(a)$

    * $\Pr \{A_t=a\}=\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}=\pi_t(a)$
    * $\pi_t(a)$表示$a$在时刻$t$被选择的概率

  * 更新

    * 选择动作$A_t$并获得收益$R_t$后进行如下更新
      $$
      \begin{align}
      H_{t+1}(A_t)&=H_t(A_t)+\alpha(R_t-\overline R_t)(1-\pi_t(A_t))\\
      H_{t+1}(a)&=H_t(a)-\alpha(R_t-\overline R_t)\pi_t(a),\ \ \ \ a\neq A_t
      \end{align}
      $$

    * $\overline R_t$是在时刻$t$内所有收益的平均值，如果收益高于均值，则未来选择的概率增大，反之减小

  * 梯度上升算法

    * 要使总体收益期望$\mathbb E[R_t]=\sum_x\pi_t(x)q_*(x)$最大

    * 有更新方程：
      $$
      \\
      H_{t+1}(a)=H_t(a)+\alpha\frac{\partial\mathbb E[R_t]}{\partial H_t(a)}\\
      $$

    * 求解梯度
      
    * $$
      \begin{align}
      \frac{\partial \mathbb E[R_t]}{\partial H_t(a)}&=\frac{\partial}{\partial H_t(a)}\Big[\sum_x\pi_t(x)q_*(x)\Big]\\
      &=\frac{\partial}{\partial H_t(a)}\Big[\sum_x\pi_t(x)\big(q_*(x)-B_t\big)\Big]\\
      &=\sum_x(q_*(x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)}\\
      &=\sum_x\pi_t(x)(q_*(x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)}/\pi_t(x)\\
      &=\mathbb E\Big[(q_*(A_t)-B_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\pi_t(A_t)\big(\mathbb 1_{a=A_t}-\pi_t(A_t)\big)/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\big(\mathbb 1_{a=A_t}-\pi_t(A_t)\big)\Big]
      \end{align}
      $$
      
    * 其中$\mathbb 1_{a=A_t}$表示如果$a=A_t$取$0$，否则取$1$

#### Chap3. 有限马尔可夫决策过程（有限Markov Decision Process, MDP）

* “环境”输出**状态**和**收益**，“智能体”输出**动作**，环境和智能体一同生成**序列/轨迹**：状态、动作、收益、状态……

* 有限MDP中，状态、动作和收益的集合都只有有限个元素，在这种情况下，**收益和状态具有明确的离散概率分布，并且只依赖于前继状态和动作。**

  * 函数$p$定义了MDP的动态特性

    * $$
      p(s',r\vert s,a)\overset{.}=\mathrm{Pr}\{S_t=s',R_t=r\vert S_{t-1}=s,A_{t-1}=a\}
      $$

  * 状态转移概率

    * $$
      p(s'|s,a)\overset{.}{=}\mathrm {Pr}\{S_t=s'\vert S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in \mathcal R}p(s',r\vert s,a)\\
      $$

  * ”状态-动作“二元组期望收益

    * $$
      r(s,a)\overset{.}{=}\mathbb E[R_t\vert S_{t-1}=a,A_{t-1}=a]=\sum_{r\in \mathcal R}r\sum_{s'\in\mathcal S}p(s',r\vert s,a)
      $$

  * “状态-动作-后继动作”三元组期望收益

    * $$
      r(s,a,s')\overset{.}=\mathbb E[R_t\vert S_{t-1}=s,A_{t-1}=a,S_t=s']=\sum_{r\in \mathcal R}r\frac{p(s',r'\vert s,a)}{p(s'\vert s,a)}
      $$

* 分幕式任务：智能体和环境的交互能被自然分成一系列子序列（每个子序列都存在最终时刻）

  * 称每个子序列为幕（episode），每幕都以一种特殊状态结束，称之为终结状态。

  * 这样的任务中可以设置回报是收益的总和

  * $$
    G_t\overset{.}=R_{t+1}+R_{t+2}+\dots+R_T
    $$

* 持续性任务：不能被自然地分为单独的幕，而是持续不断地发生

  * 由于$T=\infty$此时回报不能设置为总和，可以引入一个折扣率$\gamma$

    * $$
      G_t\overset{.}=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\\
      G_t=R_{t+1}+\gamma G_{t-1}
      $$

    * 其中$0\leq\gamma\leq1$，被称为折扣率

* 分幕式和持续性任务的统一表示法
  * 将幕单独拿出来，$S_t$表示某一幕中时刻$t$的状态（即每一幕开始时刻都是$t=0$）
  * 将分幕式任务中的终结状态当作一个特殊的吸收状态入口，即只会转移到自己并且只产生零收益（即$T=\infty$）
  * 显然$\gamma=1$时的持续性任务表示与分幕式任务一致

* 策略和价值函数

  * **价值函数**是状态（或状态与动作二元组）的函数，用来评估当前智能体在给定状态（或给定状态与动作）下有多好（用未来预期收益来定义）

  * **策略**是从状态到每个动作的选择概率之间的映射。如果智能体在时刻$t$选择了策略$\pi$，那么$\pi(a\vert s)$就是当$S_t=s$时$A_t=a$的概率

  * **状态价值函数**：将策略$\pi$下状态$s$的价值函数记为$v_\pi(s)$，即从状态$s$开始，智能体按照策略$\pi$进行决策所获得的回报的概率期望值。对于MDP 

    * $$
      v_\pi(s)\overset.=\mathbb E[G_t\vert S_t=s]=\mathbb E_\pi\Big[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\Big\vert S_t=s\Big]
      $$

  * **动作价值函数**：将策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_\pi(s,a)$。这就是根据策略$\pi$，从状态$s$开始，执行动作$a$之后，所有可能的决策序列的期望回报

    * $$
      q_\pi(s,a)\overset.=\mathbb E[G_t\vert S_t=s,A_t=a]=\mathbb E\Big[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\Big\vert S_t=s,A_t=a\Big]
      $$

  * 价值函数$v_\pi$和$q_\pi$都能从经验中估算得到，如果一个智能体遵循策略$\pi$，并且对每个遇到的状态都记录该状态后的实际回报的平均值，那么  随着状态出现次数接近无穷大，这个平均值会收敛到状态价值$v_\pi(s)$（$q_\pi(s,a)$同理）。这种估算方法称作**蒙特卡洛方法** 。

  * 对于任何策略$\pi$和任何状态$s$，$s$的价值与其可能的后继状态的价值之间存在以下关系

    * $$
      \begin{align}
      v_\pi(s)\overset.=&\mathbb E_\pi[G_t\vert S_t=s]\\
      =&\mathbb E_\pi[R_{t+1}+\gamma G_{t+1}\vert S_t=s]\\
      =&\sum_a\pi(a\vert s)\sum_{s'}\sum_rp(s',r\vert s,a)\Big[r+\gamma\mathbb E_\pi[G_{t+1}\vert S_{t+1}=s']\Big]\\
      =&\sum_a\pi(a\vert s)\sum_{s',r}p(s',r\vert s,a)\Big[r+\gamma v_\pi(s')\Big]
      \end{align}
      $$

    * 上式最后一行被称作$v_\pi$的**贝尔曼方程**，价值函数$v_\pi$是贝尔曼方程的**唯一解**
      

* 最优策略和最优价值函数

  * 对于有限MDP，可以通过比较价值函数精确地定义一个最优策略。如果要说一个策略$\pi$不比另一个策略$\pi'$差甚至比它更好，那么其所有状态上的期望回报都应该等于或大于$\pi'$的期望回报。即

    * 若对于所有$s\in\mathcal S,\ v_\pi(s)\geq v_{\pi'}(s)'$，则有$\pi\geq\pi$

  * 总会存在至少一个策略不劣于其他所有的策略，这就是**最优策略**（可能不止一个），用$\pi_{*}$来表示所有这些最优策略。他们共享相同的状态价值函数，称之为**最优状态价值函数**，记为$v_{*}$。也共享相同的**最优动作价值函数**，记为$q_{*}$

    * $$
      v_*(s)\overset.=\underset \pi\max v_\pi(s)\\
      q_*(s,a)\overset.=\underset \pi\max q_\pi(s,a)\\
      q_*(s,a)=\mathbb E[R_{t+1}+\gamma v_*(S_{t+1})\big \vert S_t=s,A_t=a]
      $$

  * 因为$v_{*}$是策略的价值函数，它必须满足贝尔曼方程中状态和价值的一致性条件。但因为它是最优的价值函数，因此$v_{*}$的一致性条件可以用一种特殊的形式表示，而不拘泥于特定的策略。这就是**贝尔曼最优方程**（最优策略下各个状态的期望一定等于这个状态下最优动作的期望回报）

    * $$
      \begin{align}
      v_*(s)&=\underset{a\in\mathcal A(s)}\max q_{\pi_*}(s,a)\\
      &=\underset a\max\mathbb E_{\pi_*}[G_t\vert S_t=s,A_t=a]\\
      &=\underset a\max\mathbb E_{\pi_*}[R_{t+1}+\gamma G_{t+1}\vert S_t=s,A_t=a]\\
      &=\underset a\max\mathbb E[R_{t+1}+\gamma v_*(S_{t+1})\vert S_t=s,A_t=a]\\
      &=\underset a\max \sum_{s',r}p(s',r\vert s,a)[r+\gamma v_*(s')]
      \end{align}
      $$

    * 最后两个等式就是$v_*$的贝尔曼最优方程的两种形式。

  * $q_*$的贝尔曼最优方程如下

    * $$
      \begin{align}
      q_*(s,a)&=\mathbb E\Big[R_{t+1}+\gamma \underset {a'}\max q_*(S_{t+1},a')\Big\vert S_t=s,A_t=a\Big]\\
      &=\sum_{s',r}p(s',r\vert s,a)[r+\gamma \underset{a'}\max q_*(s',a')]
      \end{align}
      $$

* 最优性和近似算法
  * 在状态集合小而有限的任务中，用数组或表格来估计每个状态是有可能的，这种任务称为**表格型任务**，对应的方法称作**表格型方法**
  * 但在很多实际情况下，经常有很多状态是不能用表格中的一行来表示的。因此价值函数必须采用**近似算法**，通常使用紧凑的参数化函数表示方法

* 小结
  * 智能体与环境在一连串的离散时刻进行交互。两者之间的接口定义了一个特殊的任务：
    * **动作**由智能体来选择，**状态**是做出选择的基础，而**收益**是评估选择的基础。
    * **策略**是一个智能体选择动作的随机规则，它是状态的一个函数。
