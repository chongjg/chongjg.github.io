---
layout:     post                    # 使用的布局(不需要改)
title:      《强化学习》学习笔记             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-11-02              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

## 《强化学习》学习笔记

#### Chap1. 导论

* 强化学习的基本思想
  * 在智能体为了实现目标而不断与环境产生交互的过程中，抓住智能体所面对的真实问题的主要方面。
  * 具备学习能力的智能体必须能够在某种程度上感知环境的状态，然后采取动作影响环境状态。
  * 智能体必须同时拥有和环境状态相关的一个或多个明确的目标。
* 马尔科夫决策过程包含这三个方面
  * 感知、动作、目标

* 强化学习要素
  * 策略
    * 定义了学习智能体在特定时间的行为方式，环境状态到动作的映射
  * 收益信号
    * 定义了强化学习问题中的目标，表明短时间内什么是好的（基本由环境直接给予）
  * 价值函数
    * 表明了从长远的角度看什么是好的，可理解为对将来收益累加起来的期望（需要综合评估，难以确定）
  * 对环境建立的模型
    * 给定状态和动作，模型预测外部环境的下一个状态和收益（环境模型被用于做规划）
* Marvm Minsky 在他的博士论文中（Minsky, 1954）讨论了强化学习的计算方法，描述了他组装的一 台基于模拟信号的机器，他称其为“随机神经模拟强化计算器“, SNARCs(Stochastic Neural-Analog Reinforcement Calculators)，**模拟可修改的大脑突触连接**（第15章）

#### Chap2. 多臂赌博机

* $k$臂赌博机问题

  * 学习问题：

    * 重复地在$k$个选项或动作中进行选择。每一个时刻，选择某一个动作（臂），得到一定数值收益，收益仅由选择的动作对应的平稳概率分布决定。目标是某一段时间内最大化总收益的期望

  * 动作-价值函数

    * 
      $$
      q_*(a)=\mathbb E[R_t\vert A_t=a]\\
      Q_t(a)=\frac{t时刻前执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1}R_i\cdot \mathbb 1_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb 1_{A_i=a}}
      $$

    * 贪心

      * $A_t=\underset{a}{\arg\max}\ Q_t(a)$

    * 增量式更新

      * $$
        \begin{align}
        Q_{t+1}&=\frac{1}{t}\sum_{i=1}^tR_i\\
        &=Q_t+\frac{1}{t}[R_t-Q_t]
        \end{align}
        $$

* 基于置信度上界的动作选择

  * $\epsilon-$贪心会尝试选择非贪心动作，但是是盲目的。在非贪心动作中，最好是根据他们的潜力来选择事实上是最优的动作，这要考虑动作对应的估计价值以及这些估计的不确定性（或方差）
    * $A_t=\underset{a}{\arg\max}\Big[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}\Big]$
    * $N_t(a)$表示时刻$t$之前动作$a$被选择的次数，$c$控制试探的程度

* 梯度赌博机算法

  * 偏好函数$H_t(a)$

    * $\Pr \{A_t=a\}=\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}=\pi_t(a)$
    * $\pi_t(a)$表示$a$在时刻$t$被选择的概率

  * 更新

    * 选择动作$A_t$并获得收益$R_t$后进行如下更新
      $$
      \begin{align}
      H_{t+1}(A_t)&=H_t(A_t)+\alpha(R_t-\overline R_t)(1-\pi_t(A_t))\\
      H_{t+1}(a)&=H_t(a)-\alpha(R_t-\overline R_t)\pi_t(a),\ \ \ \ a\neq A_t
      \end{align}
      $$

    * $\overline R_t$是在时刻$t$内所有收益的平均值，如果收益高于均值，则未来选择的概率增大，反之减小

  * 梯度上升算法

    * 要使总体收益期望$\mathbb E[R_t]=\sum_x\pi_t(x)q_*(x)$最大

    * 有更新方程：
      $$
      \\
      H_{t+1}(a)=H_t(a)+\alpha\frac{\part\mathbb E[R_t]}{\part H_t(a)}\\
      $$

    * 求解梯度
      $$
      \begin{align}
      \frac{\part \mathbb E[R_t]}{\part H_t(a)}&=\frac{\part}{\part H_t(a)}\Big[\sum_x\pi_t(x)q_*(x)\Big]\\
      &=\frac{\part}{\part H_t(a)}\Big[\sum_x\pi_t(x)\big(q_*(x)-B_t\big)\Big]\\
      &=\sum_x(q_*(x)-B_t)\frac{\part \pi_t(x)}{\part H_t(a)}\\
      &=\sum_x\pi_t(x)(q_*(x)-B_t)\frac{\part \pi_t(x)}{\part H_t(a)}/\pi_t(x)\\
      &=\mathbb E\Big[(q_*(A_t)-B_t)\frac{\part \pi_t(A_t)}{\part H_t(a)}/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\frac{\part \pi_t(A_t)}{\part H_t(a)}/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\pi_t(A_t)\big(\mathbb 1_{a=A_t}-\pi_t(A_t)\big)/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\big(\mathbb 1_{a=A_t}-\pi_t(A_t)\big)\Big]
      \end{align}
      $$

    * 其中$\mathbb 1_{a=A_t}$表示如果$a=A_t$取$0$，否则取$1$

#### Chap3. 
