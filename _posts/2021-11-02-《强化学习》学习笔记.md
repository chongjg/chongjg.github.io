---
layout:     post                    # 使用的布局(不需要改)
title:      《强化学习》学习笔记             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-11-02              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

## 《强化学习》学习笔记

#### Chap 1. 导论

* 强化学习的基本思想
  * 在智能体为了实现目标而不断与环境产生交互的过程中，抓住智能体所面对的真实问题的主要方面。
  * 具备学习能力的智能体必须能够在某种程度上感知环境的状态，然后采取动作影响环境状态。
  * 智能体必须同时拥有和环境状态相关的一个或多个明确的目标。
* 马尔科夫决策过程包含这三个方面
  * 感知、动作、目标

* 强化学习要素
  * 策略
    * 定义了学习智能体在特定时间的行为方式，环境状态到动作的映射
  * 收益信号
    * 定义了强化学习问题中的目标，表明短时间内什么是好的（基本由环境直接给予）
  * 价值函数
    * 表明了从长远的角度看什么是好的，可理解为对将来收益累加起来的期望（需要综合评估，难以确定）
  * 对环境建立的模型
    * 给定状态和动作，模型预测外部环境的下一个状态和收益（环境模型被用于做规划）
* Marvm Minsky 在他的博士论文中（Minsky, 1954）讨论了强化学习的计算方法，描述了他组装的一 台基于模拟信号的机器，他称其为“随机神经模拟强化计算器“, SNARCs(Stochastic Neural-Analog Reinforcement Calculators)，**模拟可修改的大脑突触连接**（第15章）

---

#### Chap 2. 多臂赌博机

* $k$臂赌博机问题

  * 学习问题：

    * 重复地在$k$个选项或动作中进行选择。每一个时刻，选择某一个动作（臂），得到一定数值收益，收益仅由选择的动作对应的平稳概率分布决定。目标是某一段时间内最大化总收益的期望

  * 动作-价值函数

    * 
      $$
      q_\ast(a)=\mathbb E[R_t\vert A_t=a]\\
      Q_t(a)=\frac{t时刻前执行动作a得到的收益总和}{t时刻前执行动作a的次数}=\frac{\sum_{i=1}^{t-1}R_i\cdot \mathbb 1_{A_i=a}}{\sum_{i=1}^{t-1}\mathbb 1_{A_i=a}}
      $$

    * 贪心

      * $A_t=\underset{a}{\arg\max}\ Q_t(a)$

    * 增量式更新

      * $$
        \begin{align}
        Q_{t+1}&=\frac{1}{t}\sum_{i=1}^tR_i\\
        &=Q_t+\frac{1}{t}[R_t-Q_t]
        \end{align}
        $$

* 基于置信度上界的动作选择

  * $\epsilon-$贪心会尝试选择非贪心动作，但是是盲目的。在非贪心动作中，最好是根据他们的潜力来选择事实上是最优的动作，这要考虑动作对应的估计价值以及这些估计的不确定性（或方差）
    * $A_t=\underset{a}{\arg\max}\Big[Q_t(a)+c\sqrt{\frac{\ln t}{N_t(a)}}\Big]$
    * $N_t(a)$表示时刻$t$之前动作$a$被选择的次数，$c$控制试探的程度

* 梯度赌博机算法

  * 偏好函数$H_t(a)$

    * $\Pr \{A_t=a\}=\frac{e^{H_t(a)}}{\sum_{b=1}^ke^{H_t(b)}}=\pi_t(a)$
    * $\pi_t(a)$表示$a$在时刻$t$被选择的概率

  * 更新

    * 选择动作$A_t$并获得收益$R_t$后进行如下更新
      $$
      \begin{align}
      H_{t+1}(A_t)&=H_t(A_t)+\alpha(R_t-\overline R_t)(1-\pi_t(A_t))\\
      H_{t+1}(a)&=H_t(a)-\alpha(R_t-\overline R_t)\pi_t(a),\ \ \ \ a\neq A_t
      \end{align}
      $$

    * $\overline R_t$是在时刻$t$内所有收益的平均值，如果收益高于均值，则未来选择的概率增大，反之减小

  * 梯度上升算法

    * 要使总体收益期望$\mathbb E[R_t]=\sum_x\pi_t(x)q_\ast(x)$最大

    * 有更新方程：
      $$
      \\
      H_{t+1}(a)=H_t(a)+\alpha\frac{\partial\mathbb E[R_t]}{\partial H_t(a)}\\
      $$

    * 求解梯度
      
    * $$
      \begin{align}
      \frac{\partial \mathbb E[R_t]}{\partial H_t(a)}&=\frac{\partial}{\partial H_t(a)}\Big[\sum_x\pi_t(x)q_\ast(x)\Big]\\
      &=\frac{\partial}{\partial H_t(a)}\Big[\sum_x\pi_t(x)\big(q_\ast(x)-B_t\big)\Big]\\
      &=\sum_x(q_\ast(x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)}\\
      &=\sum_x\pi_t(x)(q_\ast(x)-B_t)\frac{\partial \pi_t(x)}{\partial H_t(a)}/\pi_t(x)\\
      &=\mathbb E\Big[(q_\ast(A_t)-B_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\frac{\partial \pi_t(A_t)}{\partial H_t(a)}/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\pi_t(A_t)\big(\mathbb 1_{a=A_t}-\pi_t(A_t)\big)/\pi_t(A_t)\Big]\\
      &=\mathbb E\Big[(R_t-\overline R_t)\big(\mathbb 1_{a=A_t}-\pi_t(A_t)\big)\Big]
      \end{align}
      $$
      
    * 其中$\mathbb 1_{a=A_t}$表示如果$a=A_t$取$0$，否则取$1$

---

#### Chap 3. 有限马尔可夫决策过程（有限Markov Decision Process, MDP）

* “环境”输出**状态**和**收益**，“智能体”输出**动作**，环境和智能体一同生成**序列/轨迹**：状态、动作、收益、状态……

* 有限MDP中，状态、动作和收益的集合都只有有限个元素，在这种情况下，**收益和状态具有明确的离散概率分布，并且只依赖于前继状态和动作。**

  * 函数$p$定义了MDP的动态特性

    * $$
      p(s',r\vert s,a)\overset{.}=\mathrm{Pr}\{S_t=s',R_t=r\vert S_{t-1}=s,A_{t-1}=a\}
      $$

  * 状态转移概率

    * $$
      p(s'|s,a)\overset{.}{=}\mathrm {Pr}\{S_t=s'\vert S_{t-1}=s,A_{t-1}=a\}=\sum_{r\in \mathcal R}p(s',r\vert s,a)\\
      $$

  * ”状态-动作“二元组期望收益

    * $$
      r(s,a)\overset{.}{=}\mathbb E[R_t\vert S_{t-1}=a,A_{t-1}=a]=\sum_{r\in \mathcal R}r\sum_{s'\in\mathcal S}p(s',r\vert s,a)
      $$

  * “状态-动作-后继动作”三元组期望收益

    * $$
      r(s,a,s')\overset{.}=\mathbb E[R_t\vert S_{t-1}=s,A_{t-1}=a,S_t=s']=\sum_{r\in \mathcal R}r\frac{p(s',r'\vert s,a)}{p(s'\vert s,a)}
      $$

* 分幕式任务：智能体和环境的交互能被自然分成一系列子序列（每个子序列都存在最终时刻）

  * 称每个子序列为幕（episode），每幕都以一种特殊状态结束，称之为终结状态。

  * 这样的任务中可以设置回报是收益的总和

  * $$
    G_t\overset{.}=R_{t+1}+R_{t+2}+\dots+R_T
    $$

* 持续性任务：不能被自然地分为单独的幕，而是持续不断地发生

  * 由于$T=\infty$此时回报不能设置为总和，可以引入一个折扣率$\gamma$

    * $$
      G_t\overset{.}=R_{t+1}+\gamma R_{t+2}+\gamma^2R_{t+3}+\cdots=\sum_{k=0}^{\infty}\gamma^kR_{t+k+1}\\
      G_t=R_{t+1}+\gamma G_{t-1}
      $$

    * 其中$0\leq\gamma\leq1$，被称为折扣率

* 分幕式和持续性任务的统一表示法
  * 将幕单独拿出来，$S_t$表示某一幕中时刻$t$的状态（即每一幕开始时刻都是$t=0$）
  * 将分幕式任务中的终结状态当作一个特殊的吸收状态入口，即只会转移到自己并且只产生零收益（即$T=\infty$）
  * 显然$\gamma=1$时的持续性任务表示与分幕式任务一致

* 策略和价值函数

  * **价值函数**是状态（或状态与动作二元组）的函数，用来评估当前智能体在给定状态（或给定状态与动作）下有多好（用未来预期收益来定义）

  * **策略**是从状态到每个动作的选择概率之间的映射。如果智能体在时刻$t$选择了策略$\pi$，那么$\pi(a\vert s)$就是当$S_t=s$时$A_t=a$的概率

  * **状态价值函数**：将策略$\pi$下状态$s$的价值函数记为$v_\pi(s)$，即从状态$s$开始，智能体按照策略$\pi$进行决策所获得的回报的概率期望值。对于MDP 

    * $$
      v_\pi(s)\overset.=\mathbb E[G_t\vert S_t=s]=\mathbb E_\pi\Big[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\Big\vert S_t=s\Big]
      $$

  * **动作价值函数**：将策略$\pi$下在状态$s$时采取动作$a$的价值记为$q_\pi(s,a)$。这就是根据策略$\pi$，从状态$s$开始，执行动作$a$之后，所有可能的决策序列的期望回报

    * $$
      q_\pi(s,a)\overset.=\mathbb E[G_t\vert S_t=s,A_t=a]=\mathbb E\Big[\sum_{k=0}^\infty\gamma^kR_{t+k+1}\Big\vert S_t=s,A_t=a\Big]
      $$

  * 价值函数$v_\pi$和$q_\pi$都能从经验中估算得到，如果一个智能体遵循策略$\pi$，并且对每个遇到的状态都记录该状态后的实际回报的平均值，那么  随着状态出现次数接近无穷大，这个平均值会收敛到状态价值$v_\pi(s)$（$q_\pi(s,a)$同理）。这种估算方法称作**蒙特卡洛方法** 。

  * 对于任何策略$\pi$和任何状态$s$，$s$的价值与其可能的后继状态的价值之间存在以下关系

    * $$
      \begin{align}
      v_\pi(s)\overset.=&\mathbb E_\pi[G_t\vert S_t=s]\\
      =&\mathbb E_\pi[R_{t+1}+\gamma G_{t+1}\vert S_t=s]\\
      =&\sum_a\pi(a\vert s)\sum_{s'}\sum_rp(s',r\vert s,a)\Big[r+\gamma\mathbb E_\pi[G_{t+1}\vert S_{t+1}=s']\Big]\\
      =&\sum_a\pi(a\vert s)\sum_{s',r}p(s',r\vert s,a)\Big[r+\gamma v_\pi(s')\Big]
      \end{align}
      $$

    * 上式最后一行被称作$v_\pi$的**贝尔曼方程**，价值函数$v_\pi$是贝尔曼方程的**唯一解**
      

* 最优策略和最优价值函数

  * 对于有限MDP，可以通过比较价值函数精确地定义一个最优策略。如果要说一个策略$\pi$不比另一个策略$\pi'$差甚至比它更好，那么其所有状态上的期望回报都应该等于或大于$\pi'$的期望回报。即

    * 若对于所有$s\in\mathcal S,\ v_\pi(s)\geq v_{\pi'}(s)'$，则有$\pi\geq\pi$

  * 总会存在至少一个策略不劣于其他所有的策略，这就是**最优策略**（可能不止一个），用$\pi_\ast$来表示所有这些最优策略。他们共享相同的状态价值函数，称之为**最优状态价值函数**，记为$v_\ast$。也共享相同的**最优动作价值函数**，记为$q_\ast$

    * $$
      v_\ast(s)\overset.=\underset \pi\max v_\pi(s)\\
      q_\ast(s,a)\overset.=\underset \pi\max q_\pi(s,a)\\
      q_\ast(s,a)=\mathbb E[R_{t+1}+\gamma v_\ast(S_{t+1})\big \vert S_t=s,A_t=a]
      $$

  * 因为$v_\ast$是策略的价值函数，它必须满足贝尔曼方程中状态和价值的一致性条件。但因为它是最优的价值函数，因此$v_\ast$的一致性条件可以用一种特殊的形式表示，而不拘泥于特定的策略。这就是**贝尔曼最优方程**（最优策略下各个状态的期望一定等于这个状态下最优动作的期望回报）

    * $$
      \begin{align}
      v_\ast(s)&=\underset{a\in\mathcal A(s)}\max q_{\pi_\ast}(s,a)\\
      &=\underset a\max\mathbb E_{\pi_\ast}[G_t\vert S_t=s,A_t=a]\\
      &=\underset a\max\mathbb E_{\pi_\ast}[R_{t+1}+\gamma G_{t+1}\vert S_t=s,A_t=a]\\
      &=\underset a\max\mathbb E[R_{t+1}+\gamma v_\ast(S_{t+1})\vert S_t=s,A_t=a]\\
      &=\underset a\max \sum_{s',r}p(s',r\vert s,a)[r+\gamma v_\ast(s')]
      \end{align}
      $$

    * 最后两个等式就是$v_\ast$的贝尔曼最优方程的两种形式。

  * $q_\ast$的贝尔曼最优方程如下

    * $$
      \begin{align}
      q_\ast(s,a)&=\mathbb E\Big[R_{t+1}+\gamma \underset {a'}\max q_\ast(S_{t+1},a')\Big\vert S_t=s,A_t=a\Big]\\
      &=\sum_{s',r}p(s',r\vert s,a)[r+\gamma \underset{a'}\max q_\ast(s',a')]
      \end{align}
      $$

* 最优性和近似算法
  * 在状态集合小而有限的任务中，用数组或表格来估计每个状态是有可能的，这种任务称为**表格型任务**，对应的方法称作**表格型方法**
  * 但在很多实际情况下，经常有很多状态是不能用表格中的一行来表示的。因此价值函数必须采用**近似算法**，通常使用紧凑的参数化函数表示方法

* 小结
  * 智能体与环境在一连串的离散时刻进行交互。两者之间的接口定义了一个特殊的任务：
    * **动作**由智能体来选择，**状态**是做出选择的基础，而**收益**是评估选择的基础。
    * **策略**是一个智能体选择动作的随机规则，它是状态的一个函数。

---

**最优决策定理证明：**

* 定理：

  * 定义策略的偏序$\pi\geq\pi'\ \mathrm {if}\ v_\pi(s)\geq v_{\pi'}(s),\forall s$
  * 定理1：一定存在一个最优策略$\pi_*$，使得对任意策略$\pi$有$\pi_*\geq\pi$
  * 定理2：所有最优策略都有相同、最优的价值函数，即$v_{\pi_*}=v_*(s)$
  * 定理3：所有最优策略都有相同、最优的动作价值函数，即$q_{\pi_*}(s,a)=q_*(s,a)$

* 证明：

  * **贝尔曼最优方程存在唯一解证明**

    由于状态数量有限，因此价值函数可以看作是一个维度为状态集大小的价值向量
    $$
    V = \{v(s):s\in \mathcal S\}\in \mathbb R^{\vert\mathcal S\vert}
    $$
    根据最优价值函数定义推导贝尔曼最优方程：
    $$
    \begin{align}
    v_*(s)&\overset.=\underset{\pi}{\max}\ v_{\pi}(s)\nonumber\\
    &=\underset{a\in\mathcal A(s)}\max q_{\pi_*}(s,a)\nonumber\\
    &=\underset a\max\ \mathbb E_{\pi_*}[G_t\vert S_t=s,A_t=a]\nonumber\\
    &=\underset a\max\ \mathbb E_{\pi_*}[R_{t+1}+\gamma G_{t+1}\vert S_t=s,A_t=a]\nonumber\\
    &=\underset a\max\ \mathbb E[R_{t+1}+\gamma v_*(S_{t+1})\vert S_t=s,A_t=a]\nonumber\\
    &=\underset a\max\ \sum_{s',r}p(s',r\vert s,a)[r+\gamma v_*(s')]\nonumber
    \end{align}
    $$
    将贝尔曼最优方程看作是价值向量空间的映射$T$，则有
    $$
    \begin{align}
    &T:\mathbb R^{\vert\mathcal S\vert}\rightarrow\mathbb R^{\vert\mathcal S\vert}\nonumber\\
    &TV=\Big\{\underset a\max\ \sum_{s',r}p(s',r\vert s,a)[r+\gamma v(s')]:s\in\mathcal{S}\Big\}\nonumber
    \end{align}
    $$
    对于任意两个价值向量$V,V'$有
    $$
    \begin{align}
    	\parallel TV-TV'\parallel_\infty&\overset.=\underset{s\in\mathcal S}{\max}\Big\vert TV(s)-TV'(s)\Big\vert\nonumber\\
    	&=\underset{s\in\mathcal S}{\max}\Big\vert \underset a\max\ \sum_{s',r}p(s',r\vert s,a)[r+\gamma v(s')]-\underset a\max\ \sum_{s',r}p(s',r\vert s,a)[r+\gamma v'(s')]\Big\vert\nonumber\\
    	&\leq\underset{s\in\mathcal S}{\max}\ \underset a\max\ \Big\vert  \sum_{s',r}p(s',r\vert s,a)[r+\gamma v(s')]-\sum_{s',r}p(s',r\vert s,a)[r+\gamma v'(s')]\Big\vert\nonumber\\
    	&=\gamma\underset{s\in\mathcal S}{\max}\ \underset a\max\ \Big\vert  \sum_{s',r}p(s',r\vert s,a)\big[v(s')- v'(s')\big]\Big\vert\nonumber\\
    	&\leq\gamma\underset{s\in\mathcal S}{\max}\ \underset a\max\ \sum_{s',r}p(s',r\vert s,a)\Big\vert v(s')- v'(s')\Big\vert \nonumber\\
    	&\leq\gamma\underset{s\in\mathcal S}{\max}\ \underset a\max\ \sum_{s',r}p(s',r\vert s,a)\max_{s''\in \mathcal S}\Big\vert v(s'')- v'(s'')\Big\vert \nonumber\\
    	&=\gamma\max_{s''\in \mathcal S}\Big\vert v(s'')- v'(s'')\Big\vert \nonumber\\
    	&=\gamma \parallel V-V'\parallel_\infty\nonumber
    \end{align}
    $$
    由于$\gamma<1$，$T$是一个收缩映射。则对于任意$V,V'$，有
    $$
    \begin{align}
    \underset{n\rightarrow \infty}\lim\parallel T^nV-T^nV'\parallel_\infty=0\nonumber\\
    V_*=\underset{n\rightarrow \infty}\lim T^nV=\underset{n\rightarrow \infty}\lim T^nV'\nonumber\\
    TV_*=T\underset{n\rightarrow \infty}\lim T^nV=V_*\nonumber
    \end{align}
    $$
    则$V_*$为贝尔曼最优方程的解。

    假设存在多个$V_*$，则所有$\parallel{V_*}_i-V\parallel_\infty$都要随迭代同时下降，矛盾，因此$V_*$唯一。

    **综上：$v_*(s)\overset.=\underset{\pi}{\max}\ v_{\pi}(s)$存在且唯一。**

  * **贝尔曼方程存在唯一解证明**

    根据价值函数定义推导贝尔曼方程：
    $$
    \begin{align*}
    	v_\pi(s)\overset.=&\mathbb E_\pi[G_t\vert S_t=s]\\
    	=&\mathbb E_\pi[R_{t+1}+\gamma G_{t+1}\vert S_t=s]\\
    	=&\sum_a\pi(a\vert s)\sum_{s'}\sum_rp(s',r\vert s,a)\Big[r+\gamma\mathbb E_\pi[G_{t+1}\vert S_{t+1}=s']\Big]\\
    	=&\sum_a\pi(a\vert s)\sum_{s',r}p(s',r\vert s,a)\Big[r+\gamma v_\pi(s')\Big]
    \end{align*}
    $$
    类似地，将贝尔曼方程看作是价值向量空间的映射$T$，推导得到
    $$
    \begin{align}
    	\parallel TV-TV'\parallel_\infty&\overset.=\underset{s\in\mathcal S}{\max}\Big\vert TV(s)-TV'(s)\Big\vert\nonumber\\
    	&\leq\gamma \parallel V-V'\parallel_\infty\nonumber
    \end{align}
    $$
    同理得到结论：贝尔曼方程有唯一解$v(s)$

    **即：当策略$\pi$确定，则存在唯一的价值函数$v_\pi$与之对应**

  * **定理1证明**

    对任意状态$s$，找到$a_*\in\mathcal A(s)$，使得
    $$
    \begin{align}
    &\forall a'\in \mathcal A(s)\nonumber\\
    &\sum_{s',r}p(s',r\vert s,a_*)[r+\gamma v_*(s')]\geq\sum_{s',r}p(s',r\vert s,a')[r+\gamma v_*(s')]\nonumber
    \end{align}
    $$
    构造策略
    $$
    \begin{equation}
    \pi(a\vert s)=\left\{
    \begin{aligned}
    	1&,\ a=a_*\\
    	0&,\ a\neq a_*
    \end{aligned}
    \right.\nonumber
    \end{equation}
    $$
    令$v_\pi(s)=v_*(s)$

    则有
    $$
    \begin{align*}
    	v_*(s)&=\underset{a}{\max}\sum_{s',r}p(s',r\vert s,a)\Big[r+\gamma v_*(s')\Big]\\
    	&=\sum_a\pi(a\vert s)\sum_{s',r}p(s',r\vert s,a)\Big[r+\gamma v_\pi(s')\Big]\\
    \end{align*}
    $$
    满足贝尔曼方程，因此$v_*(s)$是$v_\pi(s)$的唯一解
    $$
    v_\pi(s)=v_*(s)\geq v_{\pi'}(s),\ \forall s,\pi'
    $$
    **因此，一定存在一个最优策略$\pi_*$不劣于其他任何策略**

  * **定理2证明**

    假设$\pi$对应的价值函数$v_\pi(s)\neq v_*(s)$

    根据最优价值函数的唯一性与最优性，有
    $$
    \begin{align*}
    v_*(s)\geq v_\pi(s),\ \forall s\\
    v_*(s)>v_\pi(s),\ \exists s
    \end{align*}
    $$
    因此$\pi_*>\pi$，$\pi$一定不是最优策略

    **即：最优策略$\pi_*$一定满足$v_{\pi_*}=v_*(s)$**

  * **定理3证明**

    对任意最优策略$\pi_*$，有
    $$
    v_{\pi_*}(s)=v_*(s)
    $$
    则有动作价值函数
    $$
    \begin{align*}
    	q_{\pi_*}(s,a)&\overset.=\mathbb E[G_t\vert S_t=s,A_t=a]\\
    	&=\mathbb E[R_{t+1}+\gamma v_{\pi_*}(S_{t+1})\big \vert S_t=s,A_t=a]\\
    	&=\mathbb E[R_{t+1}+\gamma v_*(S_{t+1})\big \vert S_t=s,A_t=a]\\
    	&=\underset \pi\max\ q_\pi(s,a)\\
    	&=q_*(s,a)
    \end{align*}
    $$
    **即：最优策略$\pi_*$一定满足$q_{\pi_*}=q_*(s)$**

---

#### Chap 4. 动态规划

* 动态规划（Dynamic Programming , DP）是一类优化方法，在给定一个用马尔可夫决策过程描述的完备环境模型的情况下，其可以计算最优的策略。
  * 传统DP作用有限，原因：
    * 完备的环境模型只是一个假设
    * 计算复杂度极高
  * 事实上，所有其他算法都是对DP的一种近似，只是降低了计算复杂度以及减弱了对环境模型完备性的假设
  
* 策略估计（预测）

  * 思考对于任意一个策略$\pi$，如何计算其状态价值函数$v_\pi$，这在DP文献中被称为**策略估计**
    * **贝尔曼方程唯一解的证明**中已经证明对任意策略能通过贝尔曼方程迭代收敛到唯一解。
    * 这个算法被称作**迭代策略估计**

* 策略改进

  * 考虑已有策略$\pi$，只修改某一个状态$s$的动作选择策略改为$\pi'(s,a)$并保持其他状态的策略不变，若此时有$v_{\pi'}(s)>v_\pi(s)$，则有策略$\pi'$优于$\pi$。对于修改多个状态的策略也是类似。

  * 根据原策略的价值函数执行贪心算法来构造更好策略，这个过程被称为**策略改进**。

    * $$
      \begin{align}
      \pi'(s)\overset.=\underset a {\arg\max}\ q_\pi(s,a)
      \end{align}
      $$

* 策略迭代

  * 一旦一个策略$\pi$根据$v_\pi$产生一个更好的策略$\pi'$，可以计算$v_{\pi'}$来得到一个更优的策略$\pi''$，迭代得到一个不断改进的策略和价值函数序列

    * $$
      \pi_0\overset {\mathrm E}\rightarrow v_{\pi_0}\overset {\mathrm I}\rightarrow \pi_1\overset {\mathrm E}\rightarrow v_{\pi_1}\overset {\mathrm I}\rightarrow \cdots \overset {\mathrm I}\rightarrow \pi_*\overset {\mathrm E}\rightarrow v_*
      $$

    * $\overset {\mathrm E}\rightarrow$代表策略评估，$\overset {\mathrm I}\rightarrow$代表策略改进

    * 这种寻找最优策略的方法叫作**策略迭代**

* 价值迭代

  * 策略迭代的一个缺点就是每次迭代都涉及了策略评估。但实际上在策略评估的迭代求解中，在一定步数之后即使价值函数有变化，贪心策略不一定会变化，因此可以提前结束策略评估过程。

  * 可以每次在一次遍历后就停止策略评估，这样的算法称为**价值迭代**

    * $$
      \begin{align}
      v_{k+1}\overset .=&\underset a\max\mathbb E[R_{t+1}+\gamma v_k(S_{t+1})\vert S_t=s,A_t=a]\\
      =&\underset a\max \sum_{s',r}p(s',r\vert s,a)[r+\gamma v_k(s')]
      \end{align}
      $$

* 异步动态规划

  * 前面讨论的DP方法有个主要缺点，涉及对MDP的整个状态集的操作，因此如果状态集很大，即便单次遍历也会需要非常多的时间。
  * 异步DP算法是一类就地迭代的DP算法，比如用价值迭代的更新公式，但在每一步$k$上都只更新状态$s_k$的值。

* 广义策略迭代

  * 用广义策略迭代（GPI）一词来指代让策略评估和策略改进相互作用的一般思路，与这两个流程的粒度和其他细节无关。
    * 比如策略迭代是策略评估和策略改进交替进行的，可以修改其中的一些细粒度，如提前结束策略评估的迭代。
  * 可以将GPI的评估和改进流程看作是两个约束或目标之间相互作用的流程
    * 只要在迭代的过程中$v$的改变会让$\pi$更接近$\pi_*$，$\pi$的改变让$v$更接近$v_*$，最后算法就会收敛。

* 动态规划的效率

* 小结
  * DP算法有个特殊性质：所有的方法都根据对后继状态价值的估计，来更新对当前状态价值的估计。
    * 这种普遍的思想称为**自举法**（bootstrapping）

---

#### Chap 5. 蒙特卡洛方法（Monte Carlo Methods）

* 本章中考虑第一类估计价值函数并寻找最优策略的使用算法
  * 蒙特卡洛算法仅仅需要**经验**，不需要假设拥有完备的环境知识。
    * 经验：从真实或者模拟的环境交互中采样得到的状态、动作、收益的序列。
    * 这样也就不需要关于环境动态变化规律的先验知识
  * 通过平均样本的回报来解决强化学习问题
    * 分幕式任务

* 蒙特卡洛预测

  * 给定策略学习价值函数
  * 在给定的某一幕中，状态$s$可能出现多次
    * 首次访问型MC算法用$s$的所有首次访问的回报的平均值来估计$v_\pi(s)$（本章内容）
    * 每次访问型MC算法用$s$的所有访问的回报的平均值来估计$v_\pi(s)$
  * 蒙特卡洛算法对于每个状态的估计是独立的，没有使用自举的思想
    * DP是根据后继状态的价值函数推导当前状态价值函数
    * 蒙特卡洛算法是直接根据状态回报的平均值计算价值函数

* 动作价值的蒙特卡洛估计

  * 如果无法得到环境的模型，那么计算动作价值函数更有用一些
    * 环境模型：大概可以理解为输入为状态和动作，输出为下一个状态和收益的概率分布（没有环境模型也能够在环境中采样）
    * 有模型时选特定动作使得收益与后继状态价值函数之和最大即可
    * 无模型则需要显式地确定每个动作的价值函数来确定策略
  * 求解方法与状态价值函数求解方法类似，只是变成求（状态，动作）二元组的回报的均值
  * 出现新的问题：许多二元组可能不会被访问到
    * 试探性出发（exploring starts）：将指定的二元组作为起点开始一幕的采样，同时保证所有二元组都有非零的概率被选为起点。

* 蒙特卡洛控制

  * 即如何近似最优的策略
    * 求解方式与策略迭代类似
    * 可以逐幕交替进行评估与改进，如
      * 每一幕结束后，使用观测到的回报进行策略评估，然后在每个访问到的状态进行策略改进

* 没有试探性出发假设的蒙特卡洛控制

  * 同轨策略（on-policy）/离轨策略（off-policy）：生成采样数据序列的策略和用于实际决策的待评估和改进的策略是相同/不同的
    * 离轨即生成的数据“离开”了待优化的策略所决定的决策序列轨迹
  * 同轨策略一般使用$\epsilon-$贪心策略

* 基于重要度采样的离轨策略

  * 为了搜索所有的动作，需要采取非最优的策略

    * 目标策略$\pi$：需要估计的策略
    * 行动策略$b$：采样使用的策略

  * 重要度采样

    * 希望估计的是目标策略$\pi$下的期望回报，但我们只有行动策略$b$下的回报$G_t$（回报的值相同，但是两策略下发生的概率不同）

    * 因此设置一个重要度，表示采样得到的决策序列在$\pi$下的概率与在$b$下的概率的比值

      * $$
        \rho_{t:T-1}\overset.=\frac{\Pi_{k=t}^{T-1}\pi(A_k\vert S_k)p(S_{k+1}\vert S_k,A_k)}{\Pi_{k=t}^{T-1}b(A_k\vert S_k)p(S_{k+1}\vert S_k,A_k)}=\frac{\Pi_{k=t}^{T-1}\pi(A_k\vert S_k)}{\Pi_{k=t}^{T-1}b(A_k\vert S_k)}
        $$

    * 则有$v_\pi(s)=\mathbb E[\rho_{t:T-1}G_t\vert S_t=s]$
      * 该期望可以取$\rho_{t:T-1}G_t$的均值（普通重要度采样，方差无穷大）
      * 也可以是权重为$\rho_{t:T-1}$下$G_t$的加权均值（加权重要度采样，方差逐渐收敛到零）

* 增量式实现

  * 从

    * $$
      V_n\overset.=\frac{\sum_{k=1}^{n-1}W_kG_k}{\sum_{k=1}^{n-1}W_k},\ n\geq2
      $$

  * 变为

    * $$
      V_{n+1}\overset.=V_n+\frac{W_n}{C_n}[G_n-V_n],\ n\geq1\\
      C_{n+1}\overset.=C_n+W_{n+1}
      $$

* 离轨策略蒙特卡洛控制

  * 需要选择$\epsilon-$软性的行动策略$b$

* 折扣敏感的重要度采样

  * 定义平价部分回报

    * $$
      \overline G_{t:h}\overset.=R_{t+1}+R_{t+2}+\cdots+R_T,\ 0\leq t<h\leq T
      $$

  * 传统的全回报$G_t$可以看作上述平价部分回报的加权和

    * $$
      \begin{align}
      G_t\overset.=&R_{t+1}+\gamma R_{t+2}+\gamma^2 R_{t+3}+\cdots+\gamma^{T-t-1}R_T\\
      =&(1-\gamma)R_{t+1}\\
      &+(1-\gamma)\gamma(R_{t+1}+R_{t+2})\\
      &+(1-\gamma)\gamma^2(R_{t+1}+R_{t+2}+R_{t+3})\\
      &\ \ \vdots\\
      &+(1-\gamma)\gamma^{T-t-2}(R_{t+1}+R_{t+2}+\cdots+R_{T-1})\\
      &+\gamma^{T-t-1}(R_{t+1}+R_{t+2}+\cdots+R_{T})\\
      =&(1-\gamma)\sum_{h=t+1}^{T-1}\gamma^{h-t-1}\overline G_{t:h}+\gamma^{T-t-1}\overline G_{t:T}
      \end{align}
      $$

  * 此时再对每个$\overline G_{t:h}$考虑重要度

    * 折扣敏感的普通重要度采样

      * $$
        V(s)\overset.=\frac{\sum_{t\in\mathcal T(s)}\Big((1-\gamma)\sum_{h=t+1}^{T(t)-1}\gamma^{h-t-1}\rho_{t:h-1}\overline G_{t:h}+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\overline G_{t:T(t)}\Big)}{\vert\mathcal T(s)\vert}
        $$

    * 折扣敏感的加权重要度采样

      * $$
        V(s)\overset.=\frac{\sum_{t\in\mathcal T(s)}\Big((1-\gamma)\sum_{h=t+1}^{T(t)-1}\gamma^{h-t-1}\rho_{t:h-1}\overline G_{t:h}+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\overline G_{t:T(t)}\Big)}{\sum_{t\in\mathcal T(s)}\Big((1-\gamma)\sum_{h=t+1}^{T(t)-1}\gamma^{h-t-1}\rho_{t:h-1}+\gamma^{T(t)-t-1}\rho_{t:T(t)-1}\Big)}
        $$

* 每次决策型重要度采样

  * $$
    \begin{align}
    \rho_{t:T-1}G_t=&\rho_{t:T-1}(R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{T-t-1}R_T)\\
    =&\rho_{t:T-1}R_{t+1}+\gamma\rho_{t:T-1}R_{t+2}+\cdots+\gamma^{T-t-1}\rho_{t:T-1}R_T
    \end{align}
    $$

    * $$
      \rho_{t:T-1}R_{t+1}=\frac{\pi(A_t\vert S_t)}{b(A_t\vert S_t)}\frac{\pi(A_{t+1}\vert S_{t+1})}{b(A_{t+1}\vert S_{t+1})}\frac{\pi(A_{t+2}\vert S_{t+2})}{b(A_{t+2}\vert S_{t+2})}\cdots\frac{\pi(A_{T-1}\vert S_{T-1})}{b(A_{T-1}\vert S_{T-1})}R_{t+1}
      $$

    * 在上面的式子中，可以发现只有$\frac{\pi(A_t\vert S_t)}{b(A_t\vert S_t)}$与$R_{t+1}$是相关的，（$R_{t+1}$显然只跟$t+1$时刻之前的决策有关）又

      * $$
        \mathbb E\Big[\frac{\pi(A_k\vert S_k)}{b(A_k\vert S_k)}\Big]\overset.=\sum_a b(a\vert S_k)\frac{\pi(a\vert S_k)}{b(a\vert S_k)}=\sum_a\pi(a\vert S_k)=1
        $$

    * 因此有

      * $$
        \mathbb E[\rho_{t:T-1}R_{t+1}]=\mathbb E[\rho_{t:t}R_{t+1}]
        $$

    * 类似地，对于其他项有

      * $$
        \mathbb E[\rho_{t:T-1}R_{t+k}]=\mathbb E[\rho_{t:t+k-1}R_{t+k}]
        $$

    * 综上

      * $$
        \mathbb E[\rho_{t:T-1}G_t]=\mathbb E[\overset\sim G_t]\\
        \overset\sim G_t=\sum_{k=t}^{T-1}\gamma^{k-t}\rho_{t:k}R_{k+1}
        $$

    * 这种思想称为**每次决策型重要度采样**，对于普通重要度采样器，可以使用$\overset\sim G_t$替代$G_t$后可以保持期望不变并降低方差

      * $$
        V(s)\overset.=\frac{\sum_{t\in\mathcal T(s)}\overset\sim G_t}{\vert\mathcal T(s)\vert}
        $$

* 小结

  * 相比DP的优点
    * 不需要描述环境动态特性的模型
    * 可以使用数据仿真或采样模型
    * 可以简单高效地聚焦于状态的一个小的子集
    * 在马尔可夫性不成立时性能损失较小（因为不需要自举）

* 

---



