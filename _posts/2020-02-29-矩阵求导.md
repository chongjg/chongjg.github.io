---
layout:     post                    # 使用的布局（不需要改）
title:      矩阵求导               # 标题 
subtitle:   Hello World, Hello Blog #副标题
date:       2020-02-29              # 时间
author:     chongjg                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 机器学习
    - 线性代数
---

# 矩阵求导

---

### 前言

对向量求梯度在学电磁场的时候碰到过，矩阵求导也在神经网络的反向传播里碰到，之前一直没有搞清楚，这次好好整理一下矩阵求导的规则。

### 1.布局约定(Layout conventions)

* 布局约定有分子布局(Numerator layout)和分母布局(Denominator layout)。

* 分子布局如下：

>$$\frac{\partial y}{\partial \mathbf{x}}$$为行向量，$$\frac{\partial \mathbf{y}}{\partial x}$$为列向量

* 分母布局如下：

>$$\frac{\partial y}{\partial \mathbf{x}}$$为列向量，$$\frac{\partial \mathbf{y}}{\partial x}$$为行向量

wiki上还有写第三种情况，有兴趣的戳[wiki][1]。

### 2.矩阵求导

* 该篇文章使用分子布局。

若$$\mathbf{y}=[y_1\quad y_2\quad ...\quad y_m]^T,x$$为标量，则有

>$$\quad \frac{\partial \mathbf{y}}{\partial x}=[\frac{\partial {y_1}}{\partial x}\quad \frac{\partial {y_2}}{\partial x}\quad ... \frac{\partial {y_m}}{\partial x}]^T$$

若$$\mathbf{x}=[x_1\quad x_2\quad ...\quad x_n]^T,y$$为标量，则有

>$$\quad \frac{\partial y}{\partial \mathbf{x}}=[\frac{\partial y}{\partial {x_1}}\quad \frac{\partial y}{\partial {x_2}}\quad ... \frac{\partial y}{\partial {x_n}}]$$


---

### 参考文章
[维基百科 Matrix calculus][1]

[机器学习中的线性代数之矩阵求导][2]

[矩阵的导数与迹][3]


  [1]: https://en.wikipedia.org/wiki/Matrix_calculus#Other_matrix_derivatives
  [2]: https://blog.csdn.net/u010976453/article/details/54381248
  [3]: https://www.cnblogs.com/crackpotisback/p/5545708.html
