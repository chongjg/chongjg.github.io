---
layout:     post                    # 使用的布局(不需要改)
title:      《强化学习》学习笔记（二）             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-11-29              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

## 《强化学习》学习笔记（二）

### 第二部分 表格型近似求解方法

* 在第二部分，会将第一部分的表格型方法扩展到拥有任意大的状态空间的问题上
* 在这种情况下，目标不是找到最优策略或最优价值函数，而是使用有限的计算资源找到一个比较好的近似解
* 第九章：预测问题，给定策略，去逼近其价值函数
* 第十章：控制问题，介绍最优策略的近似
* 第十一章：对离轨策略进行函数逼近
* 第十二章：资格迹
* 第十三章：策略梯度方法，直接对最优策略进行逼近，且不需要近似的价值函数

#### Chap 9. 基于函数逼近的同轨策略预测

* 价值函数逼近

  * 通过采样获取数据（状态，价值），然后使用有监督学习来近似价值函数
  * 这样的近似实际上也是一种泛化
  * 有监督学习算法需要支持在线学习，因为学习的目标函数是非平稳的

* 预测目标（$\overline{\mathrm {VE}}$）

  * 在函数逼近中，一个状态的价值估计越准确就意味着别的状态的估计不那么准确

  * 需要制定一个状态的分布$\mu(s)\geq0,\ \sum_s\mu(s)=1$来表示对每个状态$s$误差的重视程度

    * 如使用均方价值误差

    * $$
      \overline{\mathrm{VE}}(\mathbf w)\overset.=\sum_{s\in\mathcal S}\mu(s)\Big[v_\pi(s)-\hat v(s,\mathbf w)\Big]^2
      $$

    * $\mu(s)$通常定义为将在状态$s$上消耗的计算时间的比例

  * 同轨策略分幕式任务中，可令$h(s)$表示从状态$s$开始一幕交互序列的概率，则可以求解状态期望访问次数$\eta(s)$
  
    * $$
      \eta(s)=h(s)+\sum_\overline s\eta(\overline s)\sum_a\pi(a\vert \overline s)p(s\vert \overline s,a),\ \forall s\in \mathcal S\\
      \mu(s)=\frac{\eta(s)}{\sum_{s'}\eta(s')}
      $$
  
* 随机梯度和半梯度方法

  * 假设每一步观察到一个新样本$S_t\mapsto v_\pi(S_t)$，显然有

    * $$
      \begin{align}
      \mathbf w_{t+1}\overset.=&\mathbf w_t-\frac{1}{2}\alpha\nabla\Big[v_\pi(S_t)-\hat v(S_t,\mathbf w_t)\Big]^2\\
      =&\mathbf w_t+\alpha\Big[v_\pi(S_t)-\hat v(S_t,\mathbf w_t)\Big]\nabla\hat v(S_t,\mathbf w_t)
      \end{align}
      $$

  * 实际上观察到的样本可能是$S_t\mapsto U_t$，其中$U_t$是$v_\pi(S_t)$的带噪版本无偏估计

    * 因此有梯度蒙特卡洛算法，对每一步

      * $$
        \mathbf w\leftarrow \mathbf w+\alpha[G_t-\hat v(S_t,\mathbf w)]\nabla\hat v(S_t,\mathbf w)
        $$

    * 如果使用自举法，由于在n步之后的价值取决于权值向量$\mathbf w_t$的当前值，意味着它是有偏的。

      * 只包含一部分梯度，被称为**半梯度方法**

  * 状态聚合

    * 一种简单形式的泛化函数逼近
    * 状态被分到不同的组，每一个组内状态的价值估计都是同一个数

* 线性方法

  * 用向量$\mathbf x(s)$表示$s$的特征向量，特征的每一个维度都是关于$s$的函数，对于线性方法，特征被称为基函数

    * $$
      \hat v(s,\mathbf w)\overset.=\mathbf w^\mathrm T\mathbf x(s)
      $$

  * 对随机梯度有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big[U_t-\hat v(S_t,\mathbf w_t)\Big]\nabla\hat v(S_t,\mathbf w_t)=\mathbf w_t+\alpha\Big[U_t-\hat v(S_t,\mathbf w_t)\Big]\mathbf x(s)
      $$

  * 对半梯度TD(0)算法有

    * $$
      \begin{align}
      \mathbf w_{t+1}\overset.=&\mathbf w_t+\alpha\Big(R_{t+1}+\gamma\mathbf w_t^\mathrm T\mathbf x_{t+1}-\mathbf w_t^\mathrm T\mathbf x_t\Big)\mathbf x_t\\
      =&\mathbf w_t+\alpha\Big(R_{t+1}\mathbf x_t-\mathbf x_t(\mathbf x_t-\gamma\mathbf x_{t+1})^\mathrm T\mathbf w_t\Big)
      \end{align}
      $$

  * n步半梯度TD

    * $$
      \mathbf w_{t+n}\overset.=\mathbf w_{t+n-1}+\alpha\Big[G_{t:t+n}-\hat v(S_t,\mathbf w_{t+n-1})\Big]\nabla\hat v(S_t,\mathbf w_{t+n-1})\\
      G_{t:t+n}\overset.=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat v(S_{t+n},\mathbf w_{t+n-1}),\ 0\leq t\leq T-n
      $$

* 线性方法的特征构造

  * 多项式基

    * 如$\mathbf x(s)=(1,s_1,s_2,s_1s_2,s_1^2,s_2^2,\cdots)^\mathrm T$

  * 傅立叶基

    * 一维n阶傅立叶余弦基：$x_i(s)=\cos(i\pi s),\ s\in[0,1]$

  * 粗编码

    * 当状态集在一个连续二维空间上
    * 可以用圆来表示特征，如果状态在圆内，则为$1$，否则为$0$（也可以不是圆，以及不是二值）
    * 显然当更新一个状态时，覆盖它的圆所覆盖的所有状态价值也会被影响

  * 瓦片编码

    * 一种用于多维连续空间的粗编码
    * 特征的感受野组成状态空间中的一系列划分；每个划分称为一个覆盖，划分中的每个元素被称为瓦片

  * 径向基函数

    * 是粗编码在连续特征（实数）中的自然推广

    * $$
      x_i(s)\overset.=\exp\Big(-\frac{\parallel s-c_i\parallel^2}{2\sigma_i^2}\Big)
      $$

* 手动选择步长参数

  * 从基本上相同的特征向量的$\tau$次经验来学习，一个好的粗略经验法

    * $$
      \alpha\overset.=(\tau\mathbb E[\mathbf x^\mathrm T\mathbf x])^{-1}
      $$

* 非线性函数逼近：人工神经网络

* 最小二乘时序差分

  * 在前面的半梯度TD(0)算法中有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(R_{t+1}\mathbf x_t-\mathbf x_t(\mathbf x_t-\gamma\mathbf x_{t+1})^\mathrm T\mathbf w_t\Big)
      $$

  * 令$\mathbf A\overset.=\mathbb E[\mathbf x_t(\mathbf x_t-\gamma \mathbf x_{t+1})^\mathrm T],\ \mathbf b\overset.=\mathbb E[R_{t+1}\mathbf x_t]$，则有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(\mathbf b-\mathbf {Aw}_t\Big)
      $$

    * 因此有不动点$\mathbf w=\mathbf A^{-1}\mathbf b$

  * 可以直接求解期望计算不动点，而不迭代求解

    * $$
      \mathbf w_t\overset.=\hat {\mathbf A}_t^{-1}\hat{\mathbf b}_t\\
      \hat{\mathbf A}_t\overset.=\sum_{k=0}^{t-1}\mathbf x_k(\mathbf x_k-\gamma \mathbf x_{k+1})^\mathrm T+\epsilon\mathbf I\\
      \hat{\mathbf b}_t\overset.=\sum_{k=0}^{t-1}R_{t+1}\mathbf x_k
      $$

  * 上述方法虽然可以直接求解不动点，但是在新的时间步存在逆矩阵的计算，但是可以实现增量式逆矩阵更新，时间复杂度为$O(d^2)$，依然高于半梯度TD的$O(d)$复杂度

* 基于记忆的函数逼近

  * 保存看到过的训练样本（或样本子集）而不更新参数 ，当需要查询状态价值时，从基于记忆中检索查找出一组样本，然后使用这些样本来计算查询状态的价值估计值。这种方法有时被称为**懒惰学习(lazy learning)**
  * 最简单例子：最近邻居法（$s'\mapsto g$是记忆中的样本，且$s'$是离$s$最近的状态，那么$g$就作为$s$的近似值）
  * 复杂一点：加权平均法（可以让权重随着距离增加而减小）
  * 其他：局部加权回归法（回归一个曲面最小化加权误差？）

* 基于核函数的函数逼近

  * 前面的权重方法会给记忆的样本分配权值；而权值往往基于两个状态$s,s'$之间的距离，分配权值的函数我们称之为**核函数**

  * 根据距离分配权值：$\kappa:\mathbb R\rightarrow \mathbb R$

  * 根据相似度分配权值：$\kappa: \mathcal S\times\mathcal S\rightarrow\mathbb R$

  * 核函数回归基于记忆

    * $$
      \hat v(s,\mathcal D)=\sum_{s'\in\mathcal D}\kappa(s,s')g(s')
      $$

* 深入了解同轨策略学习：“兴趣”与“强调”

  * 非负随机标量变量：兴趣值$I_t$

    * 表示在时刻$t$有多大的兴趣要精确估计一个状态（或二元组）的价值

  * 非负随机标量变量：强调值$M_t$

    * 这个标量会被乘上学习过程中的更新量，因此决定了在时刻$t$强调或不强调学习

  * 则对于一般的n步学习法，有

    * $$
      \mathbf w_{t+n}\overset .=\mathbf w_{t+n-1}+\alpha M_t\Big[G_{t:t+n}-\hat v(S_t, \mathbf w_{t+n-1})\Big]\nabla\hat v(S_t,\mathbf w_{t+n-1})\\
      M_t=I_t+\gamma ^nM_{t-n}
      $$

#### Chap 10. 基于函数逼近的同轨策略控制

* 分幕式半梯度控制

  * 对于动作价值函数的梯度下降更新，样本变为$S_t,A_t\mapsto U_t$，$U_t$可以是$q_\pi(S_t,A_t)$的任意近似

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(U_t-\hat q(S_t,A_t,\mathbf w_t)\Big)\nabla\hat q(S_t,A_t,\mathbf w_t)
      $$

  * 对于单步Sarsa有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(R_{t+1}+\gamma\hat q(S_{t+1},A_{t+1},\mathbf w_{t})-\hat q(S_t,A_t,\mathbf w_t))\Big)\nabla\hat q(S_t,A_t,\mathbf w_t)
      $$

* 半梯度n步Sarsa

  * n步回报

    * $$
      G_{t:t+n}\overset.=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat q(S_{t+n},A_{t+n},\mathbf w_{t+n-1})
      $$

  * 更新公式

    * $$
      \mathbf w_{t+n}\overset .=\mathbf w_{t+n-1}+\alpha\Big[G_{t:t+n}-\hat q(S_t,A_t,\mathbf w_{t+n-1})\Big]\nabla\hat q(S_t,A_t,\mathbf w_{t+n-1})
      $$

* 平均收益：持续性任务中的新的问题设定

  * 折扣设定对函数逼近来说是有问题的，因此需要用平均收益$r(\pi)$来定义策略的质量

    * $$
      \begin{align}
      r(\pi)\overset.=&\lim_{h\rightarrow\infty}\frac{1}{h}\sum_{t=1}^h\mathbb E[R_t\vert S_0,A_{0:t-1}\sim\pi]\\
      =&\lim_{t\rightarrow \infty}\mathbb E[R_t\vert S_0,A_{0:t-1}\sim\pi]\\
      =&\sum_s \mu_\pi(s)\sum_a\pi(a\vert s)\sum_{s',r}p(s',r\vert s,a)r
      \end{align}
      $$

    * 其中$\mu_\pi(s)$是一个稳态分布，有$\mu(s)\overset.=\lim_{t\rightarrow\infty}\Pr\{S_t=s\vert A_{0:t-1}\sim\pi\}$，这意味着长远地看稳态分布只与策略本身以及MDP的转移概率相关
    
    * 认为所有达到$r(\pi)$最大值的策略都是最优的
    
  * 在平均收益设定中，回报是根据即时收益和平均收益的差来定义的

    * $$
      G_t\overset.=R_{t+1}-r(\pi)+R_{t+2}-r(\pi)+\cdots
      $$

    * 被称为**差分回报**，相应的价值函数被称为**差分价值函数**

    * $$
      v_\pi(s)\overset.=\mathbb E[G_t\vert S_t=s]\\
      q_\pi(s,a)\overset.=[G_t\vert S_t=s,A_t=a]
      $$

  * 对于两类TD误差，也有对应的差分形式

    * $$
      \delta_t\overset.=R_{t+1}-\overline R_{t+1}+\hat v(S_{t+1},\mathbf w_t)-\hat v(S_t,\mathbf w_t)\\
      \delta_t\overset.=R_{t+1}-\overline R_{t+1}+\hat q(S_{t+1},A_{t+1},\mathbf w_t)-\hat v(S_t,A_t,\mathbf w_t)
      $$

  * 对于梯度更新则有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\delta_t\nabla\hat q(S_t,A_t,\mathbf w_t)
      $$

* 弃用折扣

  * 持续性问题中折扣的无用性
  
    * 设定策略排序准则为折后价值的概率加权和
  
    * $$
      \begin{align}
      J(\pi)&=\sum_s\mu_\pi(s)v_\pi^\gamma(s)&这里v_\pi^\gamma是折后价值函数\\
      &=\sum_s\mu_\pi(s)\sum_a\pi(a\vert s)\sum_{s'}\sum_rp(s',r\vert s,a)[r+\gamma v_\pi^\gamma(s')]&贝尔曼公式\\
      &=r(\pi)+\sum_s\mu_\pi(s)\sum_a\pi(a\vert s)\sum_{s'}\sum_rp(s',r\vert s,a)\gamma v_\pi^\gamma(s')\\
      &=r(\pi)+\gamma\sum_{s'}v_\pi^\gamma(s')\sum_s\mu_\pi(s)\sum_a\pi(a\vert s)p(s'
      \vert s,a)\\
      &=r(\pi)+\gamma\sum_{s'}v_\pi^\gamma(s')\mu_\pi(s')\\
      &=r(\pi)+\gamma J(\pi)\\
      &=r(\pi)+\gamma r(\pi)+\gamma^2r(\pi)+\cdots\\
      &=\frac{1}{1-\gamma}r(\pi)
      \end{align}
      $$
  
    * 可以发现折扣率$\gamma$的改变完全不会影响到策略的大小顺序
  
    * 并且在函数逼近的折扣控制设定中，**策略改进定理不再存在**，我们在单个状态上改进折后价值函数不再保证我们会改进整个策略。
  
    * 策略改进理论的确实也是分幕式设定以及平均收益设定的理论缺陷
  
* 差分半梯度n步Sarsa

  * n步回报

    * $$
      G_{t:t+n}\overset.=R_{t+1}-\overline R_{t+1}+R_{t+2}-\overline R_{t+2}+\cdots+R_{t+n}-\overline R_{t+n}+\hat q(S_{t+n},A_{t+n},\mathbf w_{t+n-1}),&t+n<T\\
      G_{t:t+n}\overset.=G_t,&t+n\geq T
      $$

    * 其中，$\overline R$是对$r(\pi)$的估计

  * n步TD误差

    * $$
      \delta_t\overset.=G_{t:t+n}-\hat q(S_t,A_t,\mathbf w)
      $$

  * 更新

    * $$
      \delta\leftarrow \sum_{i=\tau+1}^{\tau+n}(R_i-\overline R)+\hat q(S_{\tau+n},A_{\tau+n},\mathbf w)-\hat q(S_\tau,A_\tau,\mathbf w)\\
      \overline R\leftarrow \overline R+\beta\delta\\
      \mathbf w\leftarrow\mathbf w+\alpha\delta \nabla\hat q(S_\tau,A_\tau,\mathbf w)
      $$

#### Chap 11. 基于函数逼近的离轨策略方法

* 半梯度方法

  * 重要度采样率

    * $$
      \rho_t\overset.=\rho_{t:t}=\frac{\pi(A_t\vert S_t)}{b(A_t\vert S_t)}
      $$

  * 单步状态价值函数算法：半梯度的离轨TD(0)

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\rho_t\delta_t\nabla\hat v(S_t,\mathbf w_t)
      $$

    * 其中，$\delta_t$的确切定义取决于问题是分幕式+有折扣或者持续性+无折扣

    * $$
      \begin{align}
      \delta_t\overset.=&R_{t+1}+\gamma\hat v(S_{t+1},\mathbf w_t)-\hat v(S_t,\mathbf w_t)&(分幕式任务)\\
      \delta_t\overset.=&R_{t+1}-\overline R+\hat v(S_{t+1},\mathbf w_t)-\hat v(S_t,\mathbf w_t)&(持续性任务)
      \end{align}
      $$

  * 单步动作价值函数算法：半梯度的期望Sarsa

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\delta_t\nabla\hat q(S_t,A_t,\mathbf w_t)
      $$

    * $$
      \begin{align}
      \delta_t\overset.=&R_{t+1}+\gamma\sum_a\pi(a\vert S_{t+1})\hat q(S_{t+1},a,\mathbf w_t)-\hat q(S_t,A_t,\mathbf w_t)&(分幕式任务)\\
      \delta_t\overset.=&R_{t+1}-\overline R+\sum_a\pi(a\vert S_{t+1})\hat q(S_{t+1},a,\mathbf w_t)-\hat q(S_t,A_t,\mathbf w_t)&(持续性任务)
      \end{align}
      $$

    * 注意该算法并未使用重要度采样。在表格型情形下，单步算法在更新动作价值时状态$s$和动作$a$都是确定的，因此不需要考虑其他动作。但是在使用函数逼近的情况下，就没有这么确定了，因为可能希望给不同的“状态-动作”二元组以不同的权重。

  * 多步算法中，都包含了重要度采样

  * n步Sarsa

    * $$
      \mathbf w_{t+n}\overset.=\mathbf w_{t+n-1}+\alpha\rho_{t+1}\cdots\rho_{t+n-1}[G_{t:t+n}-\hat q(S_t,A_t,\mathbf w_{t+n-1})]\nabla\hat q(S_t,A_t,\mathbf w_{t+n-1})\\
      \rho_k=1,\ k\geq T
      $$

    * $$
      \begin{align}
      G_{t:t+n}\overset.=&R_{t+1}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat q(S_{t+n},A_{t+n},\mathbf w_{t+n-1})&(分幕式任务)\\
      G_{t:t+n}\overset.=&R_{t+1}-\overline R_{t}+\cdots+R_{t+n}-\overline R_{t+n-1}+\hat q(S_{t+n},A_{t+n},\mathbf w_{t+n-1})&(持续性任务)
      \end{align}
      $$

  * 对于完全不包含重要度采样的n步树回溯算法的半梯度版本

    * $$
      \mathbf w_{t+n}\overset.=\mathbf w_{t+n-1}+\alpha[G_{t:t+n}-\hat q(S_t,A_t,\mathbf w_{t+n-1})]\nabla\hat q(S_t,A_t,\mathbf w_{t+n-1})\\
      G_{t:t+n}\overset.=\hat q(S_t,A_t,\mathbf w_{t-1})+\sum_{k=t}^{t+n-1}\delta_k\prod_{i=t+1}^k\gamma \pi(A_i\vert S_i)
      $$

    * $\delta_t$定义与上面期望Sarsa相同

* 离轨策略发散的例子

* 致命三要素

  * 只要方法同时满足下面的三个基本要素，就一定会有不稳定和发散的危险。
    * 函数逼近
    * 自举法
    * 离轨策略训练
  * 需要根据实际情况进行取舍

* 线性价值函数的几何性质

  * 假设状态空间为$\mathcal S=\{s_1,s_2,\cdots,s_{\vert \mathcal S\vert}\}$，则对于任意价值函数$v$可表示为一个向量$[v(s_1),v(s_2),\cdots,v(s_{\vert\mathcal S\vert})]^\mathrm T$
  * 考虑使用参数向量$\mathbf w$来表示价值函数，则对于最有价值函数，应当有唯一的点使得
