---
layout:     post                    # 使用的布局(不需要改)
title:      《强化学习》学习笔记（二）             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-11-29              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

## 《强化学习》学习笔记（二）

### 第二部分 表格型近似求解方法

* 在第二部分，会将第一部分的表格型方法扩展到拥有任意大的状态空间的问题上
* 在这种情况下，目标不是找到最优策略或最优价值函数，而是使用有限的计算资源找到一个比较好的近似解
* 第九章：预测问题，给定策略，去逼近其价值函数
* 第十章：控制问题，介绍最优策略的近似
* 第十一章：对离轨策略进行函数逼近
* 第十二章：资格迹
* 第十三章：策略梯度方法，直接对最优策略进行逼近，且不需要近似的价值函数

#### Chap 9. 基于函数逼近的同轨策略预测

* 价值函数逼近

  * 通过采样获取数据（状态，价值），然后使用有监督学习来近似价值函数
  * 这样的近似实际上也是一种泛化
  * 有监督学习算法需要支持在线学习，因为学习的目标函数是非平稳的

* 预测目标（$\overline{\mathrm {VE}}$）

  * 在函数逼近中，一个状态的价值估计越准确就意味着别的状态的估计不那么准确

  * 需要制定一个状态的分布$\mu(s)\geq0,\ \sum_s\mu(s)=1$来表示对每个状态$s$误差的重视程度

    * 如使用均方价值误差

    * $$
      \overline{\mathrm{VE}}(\mathbf w)\overset.=\sum_{s\in\mathcal S}\mu(s)\Big[v_\pi(s)-\hat v(s,\mathbf w)\Big]^2
      $$

    * $\mu(s)$通常定义为将在状态$s$上消耗的计算时间的比例

  * 同轨策略分幕式任务中，可令$h(s)$表示从状态$s$开始一幕交互序列的概率，则可以求解状态期望访问次数$\eta(s)$
  
    * $$
      \eta(s)=h(s)+\sum_\overline s\eta(\overline s)\sum_a\pi(a\vert \overline s)p(s\vert \overline s,a),\ \forall s\in \mathcal S\\
      \mu(s)=\frac{\eta(s)}{\sum_{s'}\eta(s')}
      $$
  
* 随机梯度和半梯度方法

  * 假设每一步观察到一个新样本$S_t\mapsto v_\pi(S_t)$，显然有

    * $$
      \begin{align}
      \mathbf w_{t+1}\overset.=&\mathbf w_t-\frac{1}{2}\alpha\nabla\Big[v_\pi(S_t)-\hat v(S_t,\mathbf w_t)\Big]^2\\
      =&\mathbf w_t+\alpha\Big[v_\pi(S_t)-\hat v(S_t,\mathbf w_t)\Big]\nabla\hat v(S_t,\mathbf w_t)
      \end{align}
      $$

  * 实际上观察到的样本可能是$S_t\mapsto U_t$，其中$U_t$是$v_\pi(S_t)$的带噪版本无偏估计

    * 因此有梯度蒙特卡洛算法，对每一步

      * $$
        \mathbf w\leftarrow \mathbf w+\alpha[G_t-\hat v(S_t,\mathbf w)]\nabla\hat v(S_t,\mathbf w)
        $$

    * 如果使用自举法，由于在n步之后的价值取决于权值向量$\mathbf w_t$的当前值，意味着它是有偏的。

      * 只包含一部分梯度，被称为**半梯度方法**

  * 状态聚合

    * 一种简单形式的泛化函数逼近
    * 状态被分到不同的组，每一个组内状态的价值估计都是同一个数

* 线性方法

  * 用向量$\mathbf x(s)$表示$s$的特征向量，特征的每一个维度都是关于$s$的函数，对于线性方法，特征被称为基函数

    * $$
      \hat v(s,\mathbf w)\overset.=\mathbf w^\mathrm T\mathbf x(s)
      $$

  * 对随机梯度有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big[U_t-\hat v(S_t,\mathbf w_t)\Big]\nabla\hat v(S_t,\mathbf w_t)=\mathbf w_t+\alpha\Big[U_t-\hat v(S_t,\mathbf w_t)\Big]\mathbf x(s)
      $$

  * 对半梯度TD(0)算法有

    * $$
      \begin{align}
      \mathbf w_{t+1}\overset.=&\mathbf w_t+\alpha\Big(R_{t+1}+\gamma\mathbf w_t^\mathrm T\mathbf x_{t+1}-\mathbf w_t^\mathrm T\mathbf x_t\Big)\mathbf x_t\\
      =&\mathbf w_t+\alpha\Big(R_{t+1}\mathbf x_t-\mathbf x_t(\mathbf x_t-\gamma\mathbf x_{t+1})^\mathrm T\mathbf w_t\Big)
      \end{align}
      $$

  * n步半梯度TD

    * $$
      \mathbf w_{t+n}\overset.=\mathbf w_{t+n-1}+\alpha\Big[G_{t:t+n}-\hat v(S_t,\mathbf w_{t+n-1})\Big]\nabla\hat v(S_t,\mathbf w_{t+n-1})\\
      G_{t:t+n}\overset.=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat v(S_{t+n},\mathbf w_{t+n-1}),\ 0\leq t\leq T-n
      $$

* 线性方法的特征构造

  * 多项式基

    * 如$\mathbf x(s)=(1,s_1,s_2,s_1s_2,s_1^2,s_2^2,\cdots)^\mathrm T$

  * 傅立叶基

    * 一维n阶傅立叶余弦基：$x_i(s)=\cos(i\pi s),\ s\in[0,1]$

  * 粗编码

    * 当状态集在一个连续二维空间上
    * 可以用圆来表示特征，如果状态在圆内，则为$1$，否则为$0$（也可以不是圆，以及不是二值）
    * 显然当更新一个状态时，覆盖它的圆所覆盖的所有状态价值也会被影响

  * 瓦片编码

    * 一种用于多维连续空间的粗编码
    * 特征的感受野组成状态空间中的一系列划分；每个划分称为一个覆盖，划分中的每个元素被称为瓦片

  * 径向基函数

    * 是粗编码在连续特征（实数）中的自然推广

    * $$
      x_i(s)\overset.=\exp\Big(-\frac{\parallel s-c_i\parallel^2}{2\sigma_i^2}\Big)
      $$

* 手动选择步长参数

  * 从基本上相同的特征向量的$\tau$次经验来学习，一个好的粗略经验法

    * $$
      \alpha\overset.=(\tau\mathbb E[\mathbf x^\mathrm T\mathbf x])^{-1}
      $$

* 非线性函数逼近：人工神经网络

* 最小二乘时序差分

  * 在前面的半梯度TD(0)算法中有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(R_{t+1}\mathbf x_t-\mathbf x_t(\mathbf x_t-\gamma\mathbf x_{t+1})^\mathrm T\mathbf w_t\Big)
      $$

  * 令$\mathbf A\overset.=\mathbb E[\mathbf x_t(\mathbf x_t-\gamma \mathbf x_{t+1})^\mathrm T],\ \mathbf b\overset.=\mathbb E[R_{t+1}\mathbf x_t]$，则有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(\mathbf b-\mathbf {Aw}_t\Big)
      $$

    * 因此有不动点$\mathbf w=\mathbf A^{-1}\mathbf b$

  * 可以直接求解期望计算不动点，而不迭代求解

    * $$
      \mathbf w_t\overset.=\hat {\mathbf A}_t^{-1}\hat{\mathbf b}_t\\
      \hat{\mathbf A}_t\overset.=\sum_{k=0}^{t-1}\mathbf x_k(\mathbf x_k-\gamma \mathbf x_{k+1})^\mathrm T+\epsilon\mathbf I\\
      \hat{\mathbf b}_t\overset.=\sum_{k=0}^{t-1}R_{t+1}\mathbf x_k
      $$

  * 上述方法虽然可以直接求解不动点，但是在新的时间步存在逆矩阵的计算，但是可以实现增量式逆矩阵更新，时间复杂度为$O(d^2)$，依然高于半梯度TD的$O(d)$复杂度

* 基于记忆的函数逼近

  * 保存看到过的训练样本（或样本子集）而不更新参数 ，当需要查询状态价值时，从基于记忆中检索查找出一组样本，然后使用这些样本来计算查询状态的价值估计值。这种方法有时被称为**懒惰学习(lazy learning)**
  * 最简单例子：最近邻居法（$s'\mapsto g$是记忆中的样本，且$s'$是离$s$最近的状态，那么$g$就作为$s$的近似值）
  * 复杂一点：加权平均法（可以让权重随着距离增加而减小）
  * 其他：局部加权回归法（回归一个曲面最小化加权误差？）

* 基于核函数的函数逼近

  * 前面的权重方法会给记忆的样本分配权值；而权值往往基于两个状态$s,s'$之间的距离，分配权值的函数我们称之为**核函数**

  * 根据距离分配权值：$\kappa:\mathbb R\rightarrow \mathbb R$

  * 根据相似度分配权值：$\kappa: \mathcal S\times\mathcal S\rightarrow\mathbb R$

  * 核函数回归基于记忆

    * $$
      \hat v(s,\mathcal D)=\sum_{s'\in\mathcal D}\kappa(s,s')g(s')
      $$

* 深入了解同轨策略学习：“兴趣”与“强调”

  * 非负随机标量变量：兴趣值$I_t$

    * 表示在时刻$t$有多大的兴趣要精确估计一个状态（或二元组）的价值

  * 非负随机标量变量：强调值$M_t$

    * 这个标量会被乘上学习过程中的更新量，因此决定了在时刻$t$强调或不强调学习

  * 则对于一般的n步学习法，有

    * $$
      \mathbf w_{t+n}\overset .=\mathbf w_{t+n-1}+\alpha M_t\Big[G_{t:t+n}-\hat v(S_t, \mathbf w_{t+n-1})\Big]\nabla\hat v(S_t,\mathbf w_{t+n-1})\\
      M_t=I_t+\gamma ^nM_{t-n}
      $$

#### Chap 10. 基于函数逼近的同轨策略控制

* 分幕式半梯度控制

  * 对于动作价值函数的梯度下降更新，样本变为$S_t,A_t\mapsto U_t$，$U_t$可以是$q_\pi(S_t,A_t)$的任意近似

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(U_t-\hat q(S_t,A_t,\mathbf w_t)\Big)\nabla\hat q(S_t,A_t,\mathbf w_t)
      $$

  * 对于单步Sarsa有

    * $$
      \mathbf w_{t+1}\overset.=\mathbf w_t+\alpha\Big(R_{t+1}+\gamma\hat q(S_{t+1},A_{t+1},\mathbf w_{t})-\hat q(S_t,A_t,\mathbf w_t))\Big)\nabla\hat q(S_t,A_t,\mathbf w_t)
      $$

* 半梯度n步Sarsa

  * n步回报

    * $$
      G_{t:t+n}\overset.=R_{t+1}+\gamma R_{t+2}+\cdots+\gamma^{n-1}R_{t+n}+\gamma^n\hat q(S_{t+n},A_{t+n},\mathbf w_{t+n-1})
      $$

  * 更新公式

    * $$
      \mathbf w_{t+n}\overset .=\mathbf w_{t+n-1}+\alpha\Big[G_{t:t+n}-\hat q(S_t,A_t,\mathbf w_{t+n-1})\Big]\nabla\hat q(S_t,A_t,\mathbf w_{t+n-1})
      $$

* 平均收益：持续性任务中的新的问题设定

  * 折扣设定对函数逼近来说是有问题的，因此需要用平均收益$r(\pi)$来代替

    * $$
      \begin{align}
      r(\pi)\overset.=&\lim_{h\rightarrow\infty}\frac{1}{h}\sum_{t=1}^h\mathbb E[R_t\vert S_0,A_{0:t-1}\sim\pi]\\
      =&\lim_{t\rightarrow \infty}\mathbb E[R_t\vert S_0,A_{0:t-1}\sim\pi]\\
      =&\sum_s \mu_\pi(s)\sum_a\pi(a\vert s)\sum_{s',r}p(s',r\vert s,a)r
      \end{align}
      $$

      

