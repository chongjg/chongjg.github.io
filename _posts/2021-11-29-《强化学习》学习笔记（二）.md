---
layout:     post                    # 使用的布局(不需要改)
title:      《强化学习》学习笔记（二）             # 标题 
subtitle:   记录新思想              #副标题
date:       2021-11-29              # 时间
author:     chongjg                 # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 学习笔记
---

## 《强化学习》学习笔记（二）

### 第二部分 表格型近似求解方法

* 在第二部分，会将第一部分的表格型方法扩展到拥有任意大的状态空间的问题上
* 在这种情况下，目标不是找到最优策略或最优价值函数，而是使用有限的计算资源找到一个比较好的近似解
* 第九章：预测问题，给定策略，去逼近其价值函数
* 第十章：控制问题，介绍最优策略的近似
* 第十一章：对离轨策略进行函数逼近
* 第十二章：资格迹
* 第十三章：策略梯度方法，直接对最优策略进行逼近，且不需要近似的价值函数

#### Chap 9. 基于函数逼近的同轨策略预测

* 价值函数逼近

  * 通过采样获取数据（状态，价值），然后使用有监督学习来近似价值函数
  * 这样的近似实际上也是一种泛化
  * 有监督学习算法需要支持在线学习，因为学习的目标函数是非平稳的

* 预测目标（$\overline{\mathrm {VE}}$）

  * 在函数逼近中，一个状态的价值估计越准确就意味着别的状态的估计不那么准确

  * 需要制定一个状态的分布$\mu(s)\geq0,\ \sum_s\mu(s)=1$来表示对每个状态$s$误差的重视程度

    * 如使用均方价值误差

    * $$
      \overline{\mathrm{VE}}(\mathbf w)\overset.=\sum_{s\in\mathcal S}\mu(s)\Big[v_\pi(s)-\hat v(s,\mathbf w)\Big]^2
      $$

    * $\mu(s)$通常定义为将在状态$s$上消耗的计算时间的比例

    *  

