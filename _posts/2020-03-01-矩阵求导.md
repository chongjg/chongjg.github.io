---
layout:     post                    # 使用的布局（不需要改）
title:      矩阵求导               # 标题 
subtitle:    #副标题
date:       2020-03-01              # 时间
author:     chongjg                      # 作者
header-img: img/post-bg-2015.jpg    #这篇文章标题背景图片
catalog: true                       # 是否归档
tags:                               #标签
    - 线性代数
    - 机器学习
---

# 矩阵求导

---

##前言

$\quad$对向量求梯度在学电磁场的时候碰到过，矩阵求导也在神经网络的反向传播里碰到，之前一直没有搞清楚，这次好好整理一下矩阵求导的规则。

###1.布局约定(Layout conventions)

$\quad$布局约定有分子布局(Numerator layout)和分母布局(Denominator layout)。

$\quad$分子布局下：

$\qquad$ $\frac{\partial y}{\partial \vec{x}}$为行向量，$\frac{\partial \vec{y}}{\partial x}$为列向量

$\quad$分母布局下：

$\qquad$ $\frac{\partial y}{\partial \vec{x}}$为列向量，$\frac{\partial \vec{y}}{\partial x}$为行向量

$\quad$wiki上还有写第三种情况，有兴趣的戳[wiki][1]。

####2.矩阵求导

$\quad$该篇文章使用分子布局。

1. $\mathbf{y}=[y_1\quad y_2\quad ...\quad y_m]^T,x$为标量，则有
$$\frac{\partial \mathbf y}{\partial x}$$

2. $\mathbf x=[x_1\quad x_2\quad ...\quad x_n]^T,y$为标量，则有
 $$$$

3. 

---




######参考文章链接
[维基百科 Matrix calculus][1]
[机器学习中的线性代数之矩阵求导][2]
[矩阵的导数与迹][3]


  [1]: https://en.wikipedia.org/wiki/Matrix_calculus#Other_matrix_derivatives
  [2]: https://blog.csdn.net/u010976453/article/details/54381248
  [3]: https://www.cnblogs.com/crackpotisback/p/5545708.html