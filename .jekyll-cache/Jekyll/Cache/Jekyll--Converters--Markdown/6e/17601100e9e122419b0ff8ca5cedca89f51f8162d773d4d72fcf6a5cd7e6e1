I"!<p>（图片截自《DeepLearning》中文版）</p>

<h2 id="第一章-引言">第一章 引言</h2>

<h4 id="1数据的不同表示对算法可能有较大影响">1.数据的不同表示对算法可能有较大影响</h4>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-1.png" alt="" /></p>

<h4 id="2可通过表示学习的方法去寻找一种较好的表示方法特征而不是手动去找比如深度学习可以直接输入图片它自动找到最好的特征">2.可通过表示学习的方法去寻找一种较好的表示方法（特征），而不是手动去找，比如深度学习可以直接输入图片它自动找到最好的特征。</h4>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-2.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-3.png" alt="" /></p>

<h2 id="第二章-线性代数">第二章 线性代数</h2>

<h4 id="1向量的l_0范数是向量的非零元素个数l_infty范数是向量元素绝对值的最大值">1.向量的<script type="math/tex">L_{0}</script>范数是向量的非零元素个数，<script type="math/tex">L_{\infty}</script>范数是向量元素绝对值的最大值。</h4>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-4.png" alt="" /></p>

<h4 id="2关于特征向量的理解">2.关于特征向量的理解</h4>

<ul>
  <li>矩阵可以看做是向量的线性变换，对于<script type="math/tex">n</script>维非零向量<script type="math/tex">\vec a,\vec b</script>，会存在线性变换（<script type="math/tex">n</script>阶方阵）<script type="math/tex">\mathbf A</script>满足</li>
</ul>

<script type="math/tex; mode=display">\mathbf A \cdot \vec a=\vec b</script>

<ul>
  <li>则矩阵<script type="math/tex">\mathbf A</script>的特征向量就可以理解为：线性变换<script type="math/tex">\mathbf A</script>只会使其进行缩放。</li>
</ul>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-5.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-6.png" alt="" /></p>

<h4 id="3奇异值分解">3.奇异值分解</h4>

<ul>
  <li><script type="math/tex">\mathbf A</script>的奇异向量、奇异值与<script type="math/tex">\mathbf A\mathbf A^T,\mathbf A^T\mathbf A</script>的特征向量、特征值之间的关系推导：</li>
</ul>

<script type="math/tex; mode=display">\mathbf A=\mathbf{UDV}^T</script>

<script type="math/tex; mode=display">\mathbf A^T=\mathbf{VD}^T\mathbf U^T</script>

<script type="math/tex; mode=display">\mathbf{AA}^T=\mathbf{UDV}^T\mathbf{VD}^T\mathbf U^T=\mathbf{UDD}^T\mathbf U^T=\mathbf Udiag(\lambda)\mathbf U^T</script>

<script type="math/tex; mode=display">\mathbf A^T\mathbf A=\mathbf{VD}^T\mathbf U^T\mathbf{UDV}^T=\mathbf{VD}^T\mathbf{DV}^T=\mathbf Vdiag(\lambda')\mathbf V^T</script>

<script type="math/tex; mode=display">\mathbf{DD}^T=diag(\lambda)</script>

<script type="math/tex; mode=display">\mathbf D^T\mathbf D=diag(\lambda')</script>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-7.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-8.png" alt="" /></p>

<h4 id="4moore-penrose伪逆">4.Moore-Penrose伪逆</h4>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-9.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-10.png" alt="" /></p>

<h4 id="5行列式">5.行列式</h4>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-11.png" alt="" /></p>

<h4 id="6主成分分析">6.主成分分析</h4>
<ul>
  <li>
    <p>主成分分析可以将数据降维，比如将有<script type="math/tex">n</script>个变量的数据只用<script type="math/tex">m</script>个变量来表示且使其尽量不丢失信息（这是因为有的变量之间是相关的）</p>
  </li>
  <li>
    <p>同样可以理解为一种精度损失较小的压缩方式，通过编码和解码进行转换。</p>
  </li>
  <li>
    <p>有<script type="math/tex">N</script>个<script type="math/tex">n</script>维向量<script type="math/tex">\vec x^{(1)}...\vec x^{(N)}</script>，要找到一个编码函数将每个<script type="math/tex">\vec x^{(i)}</script>编码成<script type="math/tex">l</script>维向量<script type="math/tex">\vec c^{(i)}</script></p>
  </li>
</ul>

<script type="math/tex; mode=display">\vec c^{(i)}=f(\vec x^{(i)})</script>

<ul>
  <li>PCA由选定的解码函数而定。例如使用简单的矩阵乘法解码：</li>
</ul>

<script type="math/tex; mode=display">g(f(\vec x^{(i)}))=\mathbf D\vec c^{(i)}</script>

<ul>
  <li>PCA的目标就是找到合适的编码函数使得<script type="math/tex">\vec x^{(i)}</script>与<script type="math/tex">g(f(\vec x^{(i)}))</script>尽可能接近。上述函数的求解方法如下：</li>
</ul>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-12.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-13.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-14.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-15.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-16.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-17.png" alt="" />
<img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-18.png" alt="" /></p>

<h2 id="第三章-概率与信息论">第三章 概率与信息论</h2>

<h4 id="概念">概念</h4>

<ul>
  <li>可数无限多：无限多个，但是可以与自然数一一对应</li>
  <li>在给定随机变量<script type="math/tex">Z</script>后，若两个随机变量<script type="math/tex">X</script>和<script type="math/tex">Y</script>是独立的，则称<script type="math/tex">X</script>和<script type="math/tex">Y</script>在给定<script type="math/tex">Z</script>时是条件独立的。</li>
  <li>协方差在某种意义上给出了两个变量线性相关性的强度以及这些变量的尺度：</li>
</ul>

<script type="math/tex; mode=display">Cov(f(x),g(y))=E[(f(x)-E[f(x)])\cdot (g(y)-E[g(y)])]</script>

<ul>
  <li>若协方差绝对值很大，则变量变化很大且同时距离各自均值的位置很远，若为正则说明倾向于同时大于或同时小于均值，若为负则说明倾向于一个大于均值另一个小于均值。
两个变量如果独立，协方差一定为零。
两个变量如果协方差为零，它们之间一定没有线性关系。</li>
  <li>中心极限定理：无穷个独立随机变量的和服从正态分布</li>
  <li>指数分布可以使<script type="math/tex">% <![CDATA[
x<0 %]]></script>时概率为<script type="math/tex">0</script></li>
</ul>

<script type="math/tex; mode=display">p(x;\lambda)=\lambda \mathbf 1_{x\geq 0}\exp(-\lambda x)</script>

<ul>
  <li>Laplace分布允许我们在任意一点<script type="math/tex">\mu</script>处设置概率质量的峰值</li>
</ul>

<script type="math/tex; mode=display">Laplace(x;\mu;\gamma)=\frac{1}{2\gamma}\exp(-\frac{|x-\mu|}{\gamma})</script>

<ul>
  <li>分布的混合：
混合分布由一些组件分布构成。每次实验，样本是由哪个组件分布产生的取决于从一个 Multinoulli 分布中采样的结果：</li>
</ul>

<script type="math/tex; mode=display">P(x)=\sum_{i}P(c=i)P(x|c=i)</script>

<ul>
  <li>
    <table>
      <tbody>
        <tr>
          <td>这里<script type="math/tex">P(c=i)</script>就是选择第<script type="math/tex">i</script>个分布的概率，<script type="math/tex">P(x\vert c=i)</script>就是第<script type="math/tex">i</script>个分布。一个非常强大且常见的混合模型是高斯混合模型（Gaussian Mixture Model）它的组件$$P(x</td>
          <td>c=i)$$是高斯分布。</td>
        </tr>
      </tbody>
    </table>
  </li>
</ul>
:ET