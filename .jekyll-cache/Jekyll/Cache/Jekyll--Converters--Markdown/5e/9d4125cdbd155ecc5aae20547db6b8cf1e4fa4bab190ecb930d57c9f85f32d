I"a<p>（所有截图及部分文字出自《DeepLearning》中文版）</p>

<ul>
  <li>以前学习神经网络真的是浅尝辄止，根本没有好好地深入研究，学习这一部分我感觉自己有很多收获。学习过程中明显感觉自己有多菜，很多地方都是思考了很久才想通或者现在还是没搞懂，但是学完后自己还是挺欣慰的，希望我的一些个人的理解能够给读者带来一点帮助。</li>
</ul>

<h2 id="第七章-深度学习中的正则化">第七章 深度学习中的正则化</h2>

<ul>
  <li>机器学习中的一个核心问题是设计不仅在训练数据上表现好，并且能在新输入上泛化好的算法。在机器学习中，许多策略显式地被设计来减少测试误差（可能会以增大训练误差为代价）。这些策略被统称为正则化。</li>
</ul>

<h3 id="一参数范数惩罚">一、参数范数惩罚</h3>

<ul>
  <li>许多正则化方法通过对目标函数<script type="math/tex">J</script>添加一个参数范数惩罚<script type="math/tex">\Omega(\mathbf \theta)</script>限制模型（如神经网络、线性回归或逻辑回归）的学习能力。我们将正则化后的目标函数记为</li>
</ul>

<script type="math/tex; mode=display">\hat J(\mathbf \theta;\mathbf X;\mathbf y)=J(\mathbf \theta;\mathbf X; \mathbf y)+\alpha \Omega(\mathbf \theta)</script>

<ul>
  <li>
    <p>其中<script type="math/tex">\alpha\in[0,\infty)</script>是权衡范数惩罚项<script type="math/tex">\Omega</script>和标准目标函数<script type="math/tex">J</script>相对贡献的超参数。<script type="math/tex">\alpha</script>设置为<script type="math/tex">0</script>表示没有正则化。<script type="math/tex">\alpha</script>越大，对应正则化惩罚越大。</p>
  </li>
  <li>
    <p>在探究不同范数的正则化表现之前，我们需要说明一下，在神经网络中，参数包括每一层仿射变换的权重和偏置，我们通常只对权重做惩罚而不对偏置做正则惩罚。</p>
  </li>
  <li>
    <p>在神经网络的情况下，有时希望对网络的每一层使用单独的惩罚，并分配不同的<script type="math/tex">\alpha</script>系数。寻找合适的多个超参数的代价很大，因此为了减少搜索空间，我们会在所有层使用相同的权重衰减。</p>
  </li>
</ul>

<h4 id="1l2参数正则化">1.L2参数正则化</h4>

<ul>
  <li>在前面已经看到过最简单而又最常见的参数范数惩罚，即通常被称为权重衰减（weight decay）的<script type="math/tex">L^2</script>参数范数惩罚。这个正则化策略通过向目标函数添加一个正则项<script type="math/tex">\Omega(\mathbf\theta)=\frac{1}{2}\left\| \mathbf \theta \right\|^2_2</script>，使权重更加接近原点。</li>
</ul>

<h4 id="2l1参数正则化">2.L1参数正则化</h4>

<ul>
  <li>
    <p><script type="math/tex">L^2</script>权重衰减是权重衰减最常见的形式，我们还可以使用其他的方法限制模型参数的规模。一个选择是使用<script type="math/tex">L^1</script>正则化。</p>
  </li>
  <li>
    <p>形式地，对模型参数<script type="math/tex">\mathbf \theta</script>的<script type="math/tex">L^1</script>正则化被定义为：</p>
  </li>
</ul>

<script type="math/tex; mode=display">\Omega(\mathbf \theta)=\left\|\mathbf \theta\right\|_1=\sum_i\vert\theta_i\vert</script>

<ul>
  <li>即各参数的绝对值之和。与<script type="math/tex">L^2</script>范数类似，正则化时不考虑偏置参数，且可以通过正超参数<script type="math/tex">\alpha</script>来控制权重衰减的强度。因此正则化的目标函数就是：</li>
</ul>

<script type="math/tex; mode=display">\overset{\sim}J(\mathbf w;\mathbf X;\mathbf y)=\alpha\left\|\mathbf w\right\|_1+J(\mathbf w;\mathbf X;\mathbf y)</script>

<ul>
  <li>
    <p>可以发现<script type="math/tex">L^1</script>正则化的效果与<script type="math/tex">L^2</script>大不一样，<script type="math/tex">L^1</script>正则化不再是线性地缩放每个参数，而是添加了一项与<script type="math/tex">\mathrm{sign}(w_i)</script>同号的常数。这样我们不一定能得到<script type="math/tex">J(\mathbf w;\mathbf X;\mathbf y)</script>二次近似的直接算数解（<script type="math/tex">L^2</script>正则化时可以）</p>
  </li>
  <li>
    <p>相比<script type="math/tex">L^2</script>正则化，<script type="math/tex">L^1</script>正则化会产生更加<strong>稀疏</strong>的解。此处稀疏性指的是最优值中的一些参数为<script type="math/tex">0</script>。</p>
  </li>
  <li>
    <p>由<script type="math/tex">L^1</script>正则化导出的稀疏性质已经被广泛地用于<strong>特征选择</strong>机制。特征选择从可用的特征子集选择出有意义的特征，化简机器学习问题，即<script type="math/tex">L^1</script>惩罚使部分子集的权重为零，表明相应的特征可以被安全地忽略。</p>
  </li>
</ul>

<h3 id="二作为约束的范数惩罚">二、作为约束的范数惩罚</h3>

<ul>
  <li>
    <p>假设现在我们想约束<script type="math/tex">\Omega(\mathbf \theta)</script>小于某个常数<script type="math/tex">k</script>。</p>
  </li>
  <li>
    <p>我们可以把参数范数惩罚看作对权重强加的约束。如果<script type="math/tex">\Omega</script>是<script type="math/tex">L^2</script>范数，那么权重就是被约束在一个<script type="math/tex">L^2</script>球中。如果<script type="math/tex">\Omega</script>是<script type="math/tex">L^1</script>范数，那么权重就是被约束在一个<script type="math/tex">L^1</script>范数限制的区域中。而且可以通过增加或减小<script type="math/tex">\alpha</script>来大致收缩或扩大约束区域。较大的<script type="math/tex">\alpha</script>，将得到一个较小的约束区域。较小的<script type="math/tex">\alpha</script>，将得到一个较大的约束区域。当然我们并不能直接从一个<script type="math/tex">\alpha</script>得到对应的<script type="math/tex">k</script>，因为他们之间的关系还取决于<script type="math/tex">J</script>的形式。</p>
  </li>
  <li>
    <p>有时候，我们希望使用显示的限制，而不是惩罚。如第4.4节所述，可以修改下降算法，使其先计算<script type="math/tex">J(\mathbf\theta)</script>的下降步，然后将<script type="math/tex">\mathbf\theta</script>投影到满足<script type="math/tex">% <![CDATA[
\Omega(\mathbf\theta)<k %]]></script>的最近点。如果我们知道什么样的<script type="math/tex">k</script>是合适的而不想花时间寻找对应此<script type="math/tex">k</script>的<script type="math/tex">\alpha</script>值，这个方法很有用。</p>
  </li>
  <li>
    <p>另一个使用显式约束和重投影而不是使用惩罚强加约束的原因是惩罚可能会导致目标函数非凸而使算法陷入局部极小（对应于小的<script type="math/tex">\mathbf\theta</script>）。重投影实现的显式约束不鼓励权重接近原点，所以通过重投影实现的显式约束只在权重变大并试图离开限制区域时产生作用。</p>
  </li>
  <li>
    <p>最后，因为重投影的显式约束还对优化过程增加了一定的稳定性，所以这是另一个好处。Hinton et al. (2012c) 建议结合使用约束和高学习速率，这样能更快地探索参数空间，并保持一定的稳定性。</p>
  </li>
  <li>
    <p>Hinton et al. (2012c) 尤其推荐由Srebro and Shraibman (2005) 引入的策略：约束神经网络层的权重矩阵每列的范数，而不是限制整个权重矩阵的 Frobenius 范数。分别限制每一列的范数可以防止某一隐藏单元有非常大的权重。在实践中，列范数的限制总是通过重投影的显式约束来实现。</p>
  </li>
</ul>

<h3 id="三正则化和欠约束问题">三、正则化和欠约束问题</h3>

<ul>
  <li>机器学习中很多线性模型，包括线性回归和PCA，都依赖于对矩阵<script type="math/tex">\mathbf X^T\mathbf X</script>求逆。只要<script type="math/tex">\mathbf X^T\mathbf X</script>是奇异的，这些方法就会失效。在这种情况下，正则化的许多形式对应求逆<script type="math/tex">\mathbf X^T\mathbf X+\alpha \mathbf I</script>。这个正则化矩阵可以保证是可逆的。</li>
</ul>

<h3 id="四数据集增强">四、数据集增强</h3>

<ul>
  <li>
    <p>让机器学习模型泛化得更好的最好办法是使用更多的数据进行训练。当然，在实践中，我们拥有的数据量是很有限的。解决这个问题的一种方法是创建假数据并 添加到训练集中。对于一些机器学习任务，创建新的假数据相当简单。</p>
  </li>
  <li>
    <p>对分类来说这种方法是最简单的。我们可以轻易通过转换训练集中的<script type="math/tex">\mathbf x</script>来生成新的<script type="math/tex">(x,y)</script>对。（不过并不是所有的任务都适用这种方法）</p>
  </li>
  <li>
    <p>数据集增强对一个具体的分类问题来说是特别有效的方法：对象识别。图像是高维的并包括各种巨大的变化因素，其中有许多可以轻易地模拟。即使模型已使用卷积和池化技术（第九章）对部分平移保持不变，沿训练图像每个方向平移几个像素的操作通常可以大大改善泛化。许多其他操作如旋转图像或缩放图像也已被证明非常有效。</p>
  </li>
  <li>
    <p>但是要注意上述操作变换图像时不要使其分类变化，比如’6’和’9’如果旋转角度太大就会使正确分类变化。</p>
  </li>
  <li>
    <p>另外在神经网络的输入层注入噪声也是常见的数据集增强方式。</p>
  </li>
</ul>

<h3 id="五半监督学习">五、半监督学习</h3>

<ul>
  <li>在半监督学习的框架下，<script type="math/tex">P(\mathbf x)</script>产生的未标记样本和<script type="math/tex">P(\mathbf x,\mathbf y)</script>中的标记样本都用于估计<script type="math/tex">P(\mathbf y\vert \mathbf x)</script>或者根据<script type="math/tex">\mathbf x</script>预测<script type="math/tex">\mathbf y</script>。</li>
</ul>

<h3 id="六多任务学习">六、多任务学习</h3>

<ul>
  <li>
    <p>多任务学习是通过合并几个任务中的样例（可以视为对参数施加的软约束）来提高泛化的一种方式。正如额外的训练样本能够将模型参数推向 具有更好泛化能力的值一样，当模型的一部分被多个额外的任务共享时，这部分将被约束为良好的值（如果共享合理），通常会带来更好的泛化能力。</p>
  </li>
  <li>
    <p>图7.2展示了多任务学习中非常普遍的一种形式，其中不同的监督任务（给定<script type="math/tex">x</script>预测<script type="math/tex">y(i)</script>）共享相同的输入<script type="math/tex">x</script>以及一些中间层表示<script type="math/tex">h^{(share)}</script>，能学习共同的因素池。该模型通常可以分为两类相关的参数：</p>
  </li>
</ul>

<ol>
  <li>具体任务的参数（只能从各自任务的样本中实现良好的泛化）。如图7.2中的上层。</li>
  <li>所有任务共享的通用参数（从所有任务的汇集数据中获益）。如图7.2中的下层。</li>
</ol>

<ul>
  <li>因为共享参数，其统计强度可大大提高（共享参数的样本数量相对于单任务模式增加的比例），并能改善泛化和泛化误差的范围(Baxter, 1995)。当然，仅当不同的任务之间存在某些统计关系的假设是合理（意味着某些参数能通过不同任务共享） 时才会发生这种情况。</li>
</ul>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-37.png" alt="" /></p>

<h3 id="七提前终止">七、提前终止</h3>

<ul>
  <li>
    <p>当训练有足够的表示能力甚至会过拟合的大模型时，我们经常观察到，训练误差会随着时间的推移逐渐降低但验证集的误差会再次上升。图7.3是这些现象的一个例子，这种现象几乎一定会出现。</p>
  </li>
  <li>
    <p>当验证集上的误差在事先指定的循环次数内没有进一步改善时，算法就会终止。并且算法返回的是使验证集误差最低的参数设置。这种策略被称为<strong>提前终止</strong>。这可能是深度学习中最常用的正则化形式。它的流行主要是因为有效性和简单性。</p>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-38.png" alt="" /></p>

<ul>
  <li>
    <p>提前终止是一种非常不显眼的正则化形式，它几乎不需要改变基本训练过程、目标函数或一组允许的参数值。这意味着，无需破坏学习动态就能很容易地使用提前终止。相对于权重衰减，必须小心不能使用太多的权重衰减，以防网络陷入不良局部极小点(对应于病态的小权重)。</p>
  </li>
  <li>
    <p>提前终止需要验证集，这意味着某些训练数据不能被馈送到模型。为了更好地利用这一额外的数据，我们可以在完成提前终止的首次训练之后，进行额外的训练。在第二轮，即额外的训练步骤中，所有的训练数据都被包括在内。有两个基本的策略都可以用于第二轮训练过程。</p>
  </li>
  <li>
    <p>一个策略是再次初始化模型，然后使用所有数据再次训练。在这个第二轮训练过程中，我们使用第一轮提前终止训练确定的最佳步数。此过程有一些细微之处。例如，我们没有办法知道重新训练时，对参数进行相同次数的更新和对数据集进行相同次数的遍历哪一个更好。由于训练集变大了，在第二轮训练时，每一次遍历数据集将会更多次地更新参数。</p>
  </li>
  <li>
    <p>另一个策略是保持从第一轮训练获得的参数，然后使用全部的数据继续训练。在这个阶段，已经没有验证集指导我们需要在训练多少步后终止。取而代之，我们可以监控验证集的平均损失函数，并继续训练，直到它低于提前终止过程终止时的目标值。此策略避免了重新训练模型的高成本，但表现并没有那么好。例如，验证集的目标不一定能达到之前的目标值，所以这种策略甚至不能保证终止。</p>
  </li>
</ul>

<h3 id="八参数绑定和参数共享">八、参数绑定和参数共享</h3>

<ul>
  <li>
    <p>我们经常想要表达的一种常见依赖是某些参数应当彼此接近。考虑以下情形：我们有两个模型执行相同的分类任务（具有相同类别），但输入分布稍有不同。</p>
  </li>
  <li>
    <p>我们可以想象，这些任务会足够相似（或许具有相似的输入和输出分布），因此我们认为模型参数应彼此靠近，具体地，我们可以使用以下形式的参数范数惩罚：</p>
  </li>
</ul>

<script type="math/tex; mode=display">\Omega(\mathbf w^{(A)},\mathbf w^{(B)})=\left \| w^{(A)}-w^{(B)}\right \|^2_2</script>

<ul>
  <li>参数范数惩罚是正则化参数使其彼此接近的一种方式，而更流行的方法是使用约束：强迫某些参数相等。由于我们将各种模型或模型组件解释为共享唯一的一组参数，这种正则化方法通常被称为<strong>参数共享</strong>。对于某些特定模型，如卷积神经网络，这可能可以显著减少模型所占用的内存。</li>
</ul>

<h4 id="1卷积神经网络">1.卷积神经网络</h4>

<ul>
  <li>
    <p>目前为止，最流行和广泛使用的参数共享出现在应用于计算机视觉的卷积神经网络（CNN）中。</p>
  </li>
  <li>
    <p>自然图像有许多统计属性是对转换不变的。例如，猫的照片即使向右边移了一个像素，仍保持猫的照片。CNN通过在图像多个位置共享参数来考虑这个特性。相同的特征（具有相同权重的隐藏单元）在输入的不同位置上计算获得。这意味着无论猫出现在图像中的第<script type="math/tex">i</script>列或<script type="math/tex">i+1</script>列，我们都可以使用相同的猫探测器找到猫。</p>
  </li>
  <li>
    <p>参数共享显著降低了CNN模型的参数数量，并显著提高了网络的大小而不需要 相应地增加训练数据。它仍然是将领域知识有效地整合到网络架构的最佳范例之一。</p>
  </li>
</ul>

<h3 id="九稀疏表示">九、稀疏表示</h3>

<ul>
  <li>
    <p>前文所述的权重衰减直接惩罚模型参数。另一种策略是惩罚神经网络中的激活单元，稀疏化激活单元。这种策略间接地对模型参数施加了复杂惩罚。</p>
  </li>
  <li>
    <p>我们已经讨论过（在第7.1.2节中）<script type="math/tex">L1</script>惩罚如何诱导稀疏的参数，即许多参数为零（或接近于零）。另一方面，表示的稀疏描述了许多元素是零（或接近零）的表示。</p>
  </li>
  <li>
    <p>还有一些其他方法通过激活值的硬性约束来获得表示稀疏。例如，<strong>正交匹配追踪</strong> (orthogonal matching pursuit)(Pati et al., 1993) 通过解决以下约束优化问题将输入值<script type="math/tex">\mathbf x</script>编码成表示<script type="math/tex">\mathbf h</script></p>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\underset{\mathbf h,\left\|\mathbf h\right\|_0<k}{\mathrm{arg\min}}\left\|\mathbf x-\mathbf{Wh}\right\|^2 %]]></script>

<ul>
  <li>
    <p>其中<script type="math/tex">\left\|\mathbf h\right\|_0</script>是<script type="math/tex">\mathbf h</script>中非零项的个数。当<script type="math/tex">\mathbf W</script>被约束为正交时，我们可以高效地解决这个问题。这种方法通常被称为OMP-k，通过<script type="math/tex">k</script>指定允许的非零特征数量。Coates and Ng (2011) 证明OMP-1可以成为深度架构中非常有效的特征提取器。</p>
  </li>
  <li>
    <p>含有隐藏单元的模型在本质上都能变得稀疏。</p>
  </li>
</ul>

<h3 id="十bagging-和其他集成方法">十、Bagging 和其他集成方法</h3>

<ul>
  <li>
    <p><strong>Bagging</strong>（bootstrap aggregating）是通过结合几个模型降低泛化误差的技术(Breiman, 1994)。主要想法是分别训练几个不同的模型，然后让所有模型表决测试样例的输出。这是机器学习中常规策略的一个例子，被称为<strong>模型平均</strong>（model averaging）。采用这种策略的技术被称为集成方法。</p>
  </li>
  <li>
    <p><strong>模型平均</strong>（model averaging）奏效的原因是不同的模型通常不会在测试集上产生完全相同的误差。</p>
  </li>
  <li>
    <p>模型平均是一个减少泛化误差的非常强大可靠的方法。在作为科学论文算法的基准时，它通常是不鼓励使用的，因为任何机器学习算法都可以从模型平均中大幅获益（以增加计算和存储为代价）。</p>
  </li>
  <li>
    <p>机器学习比赛中的取胜算法通常是使用超过几十种模型平均的方法。最近一个突出的例子是Netflix Grand Prize(Koren, 2009)。</p>
  </li>
</ul>

<h3 id="十一dropout">十一、Dropout</h3>

<ul>
  <li>
    <p><strong>Dropout</strong> (Srivastava et al., 2014) 提供了正则化一大类模型的方法，计算方便但功能强大。在第一种近似下，Dropout可以被认为是集成大量深层神经网络的实 用Bagging方法。Bagging涉及训练多个模型，并在每个测试样本上评估多个模型。 当每个模型都是一个很大的神经网络时，这似乎是不切实际的，因为训练和评估这样的网络需要花费很多运行时间和内存。通常我们只能集成五至十个神经网络，如Szegedy et al. (2014a)集成了六个神经网络赢得 ILSVRC，超过这个数量就会迅速变得难以处理。Dropout提供了一种廉价的Bagging集成近似，能够训练和评估指数级数量的神经网络。</p>
  </li>
  <li>
    <p>具体而言，Dropout训练的集成包括所有从基础网络除去非输出单元后形成的子网络，如图7.6所示。</p>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-39.png" alt="" /></p>

<ul>
  <li>
    <p>我们将Dropout介绍为一种纯粹高效近似Bagging的方法。然而，还有比这更进一步的Dropout观点。Dropout不仅仅是训练一个Bagging的集成模型，并且是共享隐藏单元的集成模型。这意味着无论其他隐藏单元是否在模型中，每个隐藏单元必须都能够表现良好。隐藏单元必须准备好进行模型之间的交换和互换。</p>
  </li>
  <li>
    <p>Dropout强大的大部分原因来自施加到隐藏单元的掩码噪声，了解这一事实是重要的。这可以看作是对输入内容的信息高度智能化、自适应破坏的一种形式，而不是对输入原始值的破坏。例如，如果模型学得通过鼻检测脸的隐藏单元<script type="math/tex">h_i</script>，那么丢失<script type="math/tex">h_i</script> 对应于擦除图像中有鼻子的信息。模型必须学习另一种<script type="math/tex">h_i</script>，要么是鼻子存在的冗余编码，要么是像嘴这样的脸部的另一特征。传统的噪声注入技术，在输入端加非结构化的噪声不能够随机地从脸部图像中抹去关于鼻子的信息，除非噪声的幅度大到几乎能抹去图像中所有的信息。破坏提取的特征而不是原始值，让破坏过程充分利用该模型迄今获得的关于输入分布的所有知识。</p>
  </li>
  <li>
    <p>Dropout的另一个重要方面是噪声是乘性的。如果是固定规模的加性噪声，那么加了噪声<script type="math/tex">\epsilon</script>的整流线性隐藏单元可以简单地学会使<script type="math/tex">h_i</script>变得很大（使增加的噪声<script type="math/tex">\epsilon</script>变得不显著）。乘性噪声不允许这样病态地解决噪声鲁棒性问题。</p>
  </li>
  <li>
    <p>另一种深度学习算法——批标准化，在训练时向隐藏单元引入加性和乘性噪声重新参数化模型。批标准化的主要目的是改善优化，但噪声具有正则化的效果，有时没必要再使用Dropout。</p>
  </li>
</ul>

<h3 id="十二对抗训练">十二、对抗训练</h3>

<ul>
  <li>
    <p>在许多情况下，神经网络在独立同分布的测试集上进行评估已经达到了人类表现。因此，我们自然要怀疑这些模型在这些任务上是否获得了真正的人类层次的理解。为了探索网络对底层任务的理解层次，我们可以探索这个模型错误分类的例子。Szegedy et al. (2014b) 发现，在精度达到人类水平的神经网络上通过优化过程故意构造数据点，其上的误差率接近100%，模型在这个输入点<script type="math/tex">x'</script>的输出与附近的数据点<script type="math/tex">x</script>非常不同。在许多情况下，<script type="math/tex">x'</script>与<script type="math/tex">x</script>非常近似，人类观察者不会察觉原始样本和对抗样本（adversarial example）之间的差异，但是网络会作出非常不同的预测。</p>
  </li>
  <li>
    <p>一个小idea：把每个参数的梯度绝对值加入损失函数</p>
  </li>
  <li>
    <p>对抗样本也提供了一种实现半监督学习的方法。在与数据集中的标签不相关联的点<script type="math/tex">x</script>处，模型本身为其分配一些标签<script type="math/tex">\hat y</script>。模型的标记<script type="math/tex">\hat y</script>未必是真正的标签，但如果模型是高品质的，那么<script type="math/tex">\hat y</script>提供正确标签的可能性很大。我们可以搜索一个对抗样本<script type="math/tex">x'</script>，导致分类器输出一个标签<script type="math/tex">y'</script>且<script type="math/tex">y'\neq \hat y</script>。不使用真正的标签，而是由训练好 的模型提供标签产生的对抗样本被称为虚拟对抗样本。我们可以训练分类器为<script type="math/tex">x</script>和<script type="math/tex">x'</script>分配相同的标签。这鼓励分类器学习一个沿着未标签数据所在流形上任意微小变化都很鲁棒的函数。驱动这种方法的假设是，不同的类通常位于分离的流形上，并且小扰动不会使数据点从一个类的流形跳到另一个类的流形上。</p>
  </li>
</ul>

<h3 id="十三切面距离正切传播和流形正切分类器">十三、切面距离、正切传播和流形正切分类器</h3>
:ET