I"^?<p>（所有截图及部分文字出自《DeepLearning》中文版）</p>

<ul>
  <li>以前学习神经网络真的是浅尝辄止，根本没有好好地深入研究，在这一部分我感觉自己收获了很多，也希望我的一些理解能够给读者带来一点帮助。</li>
</ul>

<h2 id="第六章-深度前馈网络">第六章 深度前馈网络</h2>

<p>关于神经网络入门有个系列视频非常推荐：</p>

<blockquote>
  <p><a href="https://www.bilibili.com/video/av15532370">深度学习之神经网络的结构</a><br />
<a href="https://www.bilibili.com/video/av16144388/?spm_id_from=333.788.videocard.0">深度学习之梯度下降法</a><br />
<a href="https://www.bilibili.com/video/av16577449/?spm_id_from=333.788.videocard.0">深度学习之反向传播算法 上</a><br />
<a href="https://www.bilibili.com/video/av16577449?p=2">深度学习之反向传播算法 下</a></p>
</blockquote>

<ul>
  <li>
    <p><strong>深度前馈网络</strong>，也叫作<strong>前馈神经网络</strong>或者<strong>多层感知机</strong></p>
  </li>
  <li>
    <p>模型被称为<strong>前向</strong>是因为输出与模型之间没有<strong>反馈</strong>连接。当包含反馈连接时，被称为<strong>循环神经网络</strong>，在第十章将会提到。</p>
  </li>
  <li>
    <p>前馈网络最后一层称为<strong>输出层</strong>。由于训练数据没有给出这些层中每一层所需的输出，因此这些层被称为<strong>隐藏层</strong>。</p>
  </li>
  <li>
    <p>可以把层想象成由许多并行操作的<strong>单元</strong>组成，每个单元表示一个向量到标量的函数。</p>
  </li>
  <li>
    <p>在神经网络发展初期，曾经就被数学家用无法学习XOR函数怼过，因为普通的神经网络是线性模型，说到底还是多个矩阵连乘，为了解决这个问题<strong>整流线性单元</strong>出现了，这个激活函数实际上就是</p>
  </li>
</ul>

<script type="math/tex; mode=display">g(z)=max\{0,z\}</script>

<h3 id="一代价函数">一、代价函数</h3>

<h4 id="1使用最大似然学习条件分布">1.使用最大似然学习条件分布</h4>

<ul>
  <li>可以使用最大似然来训练，代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。该函数表示为：</li>
</ul>

<script type="math/tex; mode=display">J(\mathbf \theta)=-\mathbb E_{\mathbf{x,y}\sim\hat p_{data}}\log p_{model}(\mathbf y\vert \mathbf x)</script>

<ul>
  <li>上述方程展开后通常会有一些项不依赖于模型的参数。比如若有</li>
</ul>

<script type="math/tex; mode=display">p_{model}(\mathbf y\vert\mathbf x)=\mathcal{N}(\mathbf y\vert f(\mathbf x;\mathbf \theta),\mathbf I)</script>

<ul>
  <li>那么我们就重新得到了均方误差代价（高斯分布函数取对数之后是均方误差）</li>
</ul>

<script type="math/tex; mode=display">J(\mathbf \theta)=\frac{1}{2}\mathbb E_{\mathbf{x,y}\sim \hat p_{data}}\vert\vert\mathbf y-f(\mathbf x;\mathbf \theta)\vert\vert^2+const</script>

<ul>
  <li>使用最大似然导出代价函数的一个优势就是，减轻了为模型设计代价函数的负担，因为只要明确一个模型<script type="math/tex">p(\mathbf y\vert\mathbf x)</script>就自动地确定了一个代价函数<script type="math/tex">\log p(\mathbf y\vert\mathbf x)</script>。</li>
</ul>

<h4 id="2学习条件统计量">2.学习条件统计量</h4>

<ul>
  <li>
    <p>有时我们并不想学习一个完整的概率分布<script type="math/tex">p(\mathbf y\vert \mathbf x)</script>，而仅仅是想学习在给定<script type="math/tex">\mathbf x</script>时<script type="math/tex">\mathbf y</script>的某个条件统计量。</p>
  </li>
  <li>
    <p>例如，有一个预测器<script type="math/tex">f(\mathbf x;\mathbf \theta)</script>，我们想用它来预测<script type="math/tex">\mathbf y</script>的均值。</p>
  </li>
  <li>
    <p>我们可以认为神经网络能够表示一大类函数中任何一个函数<script type="math/tex">f</script>，可以把代价函数看作是一个<strong>泛函</strong>，泛函是函数到实数的映射，也就是要学习一个函数使得代价最小。</p>
  </li>
  <li>
    <p>可以使用<strong>变分法</strong>导出第一个结果是解优化问题（这里不知道变分法是啥。。在十九章会学习）</p>
  </li>
</ul>

<script type="math/tex; mode=display">f^*=\underset{f}{\mathrm{argmin}}\mathbb E_{\mathbf{x,y}\sim p_{data}}\vert\vert\mathbf y-f(\mathbf x)\vert\vert^2</script>

<ul>
  <li>得到</li>
</ul>

<script type="math/tex; mode=display">f^*(\mathbf x)=\mathbb E_{\mathbf y\sim p_{data}(\mathbf y\vert\mathbf x)}[\mathbf y]</script>

<ul>
  <li>
    <p>要求这个函数处在我们要优化的类里。</p>
  </li>
  <li>
    <p>不同代价函数给出不同的统计量。第二个会用变分法得到的结果是</p>
  </li>
</ul>

<script type="math/tex; mode=display">f^*=\underset{f}{\mathrm{argmin}}\mathbb E_{\mathbf{x,y}\sim p_{data}}\vert\vert\mathbf y-f(\mathbf x)\vert\vert_1</script>

<ul>
  <li>
    <p>将得到一个函数可以对每个<script type="math/tex">\mathbf x</script>预测<script type="math/tex">\mathbf y</script>取值的中位数，只要这个函数在我们要优化的函数族里。这个代价函数通常被称为<strong>平均绝对误差</strong></p>
  </li>
  <li>
    <p><strong>均方误差</strong>和<strong>平均绝对误差</strong>在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元结合这些代价函数会产生很小的梯度。这是<strong>交叉熵</strong>代价函数比他们更受欢迎的原因之一，即使是在没有必要估计整个<script type="math/tex">p(\mathbf y\vert\mathbf x)</script>分布时。</p>
  </li>
</ul>

<h3 id="二输出单元">二、输出单元</h3>

<ul>
  <li>
    <p>代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。</p>
  </li>
  <li>
    <p>任何可用作输出的神经网络单元，也可以被用作隐藏单元。</p>
  </li>
  <li>
    <p>本节中，假设前馈网络提供了一组定义为<script type="math/tex">\mathbf h=f(\mathbf x;\mathbf \theta)</script>的隐藏特征。输出层的作用是随后对这些特征进行额外的变换来完成整个网络必须完成的任务。</p>
  </li>
</ul>

<h4 id="1用于高斯输出分布的线性单元">1.用于高斯输出分布的线性单元</h4>

<ul>
  <li>
    <p>一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性，这些单元往往被称为线性单元。</p>
  </li>
  <li>
    <p>给定特征<script type="math/tex">\mathbf h</script>，线性输出层产生一个向量<script type="math/tex">\hat{\mathbf y}=\mathbf W^T\mathbf h+b</script></p>
  </li>
  <li>
    <p>线性输出层经常被用来产生条件高斯分布的均值：</p>
  </li>
</ul>

<script type="math/tex; mode=display">p(\mathbf y\vert\mathbf x)=\mathcal N(\mathbf y;\hat{\mathbf y},\mathbf I)</script>

<ul>
  <li>
    <p>最大化其对数似然此时等价于最小化均方误差。</p>
  </li>
  <li>
    <p>因为线性模型不会饱和，所以易于采用基于梯度的优化算法，甚至可以使用其他多种优化算法。</p>
  </li>
</ul>

<h4 id="2用于bernoulli输出分布的sigmoid单元">2.用于Bernoulli输出分布的sigmoid单元</h4>

<ul>
  <li>
    <p>许多任务需要预测二值型变量<script type="math/tex">y</script>的值。具有两个类的分类问题可以归结为这种形式。</p>
  </li>
  <li>
    <p>神经网络只需要预测<script type="math/tex">P(y=1\vert \mathbf x)</script>即可。为了使这个数是有效的概率，它必须处在区间<script type="math/tex">[0,1]</script>中。</p>
  </li>
  <li>
    <p>如果使用线性单元对<script type="math/tex">0,1</script>分别取最大值和最小值，则梯度下降法求导时会变得更复杂。而且在区间外时梯度直接变成<script type="math/tex">0</script>，这样就无法进行学习了。</p>
  </li>
  <li>
    <p>需要一种新方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用<script type="math/tex">\mathrm{sigmoid}</script>输出单元结合最大似然来实现的。</p>
  </li>
  <li>
    <p><script type="math/tex">\mathrm{sigmoid}</script>输出单元定义为</p>
  </li>
</ul>

<script type="math/tex; mode=display">\hat y=\sigma(\mathbf w^T\mathbf h+b)</script>

<ul>
  <li>这里<script type="math/tex">\sigma</script>是第三章中介绍的<script type="math/tex">logistic\;\mathrm{sigmoid}</script>函数</li>
</ul>

<script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+\exp(-x)}</script>

<ul>
  <li>
    <p>我们可以将<script type="math/tex">\mathrm{sigmoid}</script>输出单元看成线性层<script type="math/tex">z=\mathbf w^T\mathbf h+b</script>和<script type="math/tex">\mathrm{sigmoid}</script>激活函数将<script type="math/tex">z</script>转化成概率。</p>
  </li>
  <li>
    <p>我们暂时忽略对于<script type="math/tex">\mathbf x</script>的依赖性，只讨论<script type="math/tex">z</script>的值来定义<script type="math/tex">y</script>的概率分布。</p>
  </li>
  <li>
    <p>首先构造一个非归一化（和不为<script type="math/tex">1</script>）的概率分布<script type="math/tex">\tilde P(y)</script>，假定其对数概率为</p>
  </li>
</ul>

<script type="math/tex; mode=display">\log \tilde P(y)=yz</script>

<ul>
  <li>取指数得到非归一化概率</li>
</ul>

<script type="math/tex; mode=display">\tilde P(y)=\exp(yz)</script>

<ul>
  <li>上面两个公式个人理解是赋予网络输出<script type="math/tex">z</script>一个合理的意义，可以观察到非归一化概率<script type="math/tex">\tilde P(y=0)=1,\tilde P(y=1)=\exp(z)</script>，也就是通过<script type="math/tex">z</script>的大小作为二值分布概率的相对差别。归一化可得</li>
</ul>

<script type="math/tex; mode=display">P(y)=\frac{\tilde P(y)}{\sum_{y'=0}^1\tilde P(y=y')}=\frac{\exp(yz)}{\sum_{y'=0}^{1}\exp(y'z)}</script>

<script type="math/tex; mode=display">P(y)=\sigma((2y-1)z)</script>

<ul>
  <li>
    <p>上面第二个公式乍一看有点跳跃，实际上把<script type="math/tex">y=0,1</script>分别代入可以发现是正确的。</p>
  </li>
  <li>
    <p>用于定义这种二值型变量分布的变量<script type="math/tex">z</script>被称为<strong>分对数</strong>。</p>
  </li>
  <li>
    <p>这样很自然地可以使用最大似然代价函数</p>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
J(\mathbf \theta)=&-\log P(y\vert \mathbf x)\\
=&-\log \sigma((2y-1)z)\\
=&\zeta((1-2y)z)
\end{align} %]]></script>

<ul>
  <li>
    <p><script type="math/tex">\zeta</script>就是<script type="math/tex">\mathrm{softplus}</script>函数，<script type="math/tex">\zeta(x)=\log(1+\exp(x))</script>，可以看做是平滑版的<script type="math/tex">\mathrm{Relu}</script>函数。而且由于函数类似于<script type="math/tex">\mathrm{Relu}</script>，在<script type="math/tex">% <![CDATA[
x<0 %]]></script>时梯度会快速衰减到很小，在<script type="math/tex">x>0</script>时梯度不会收缩。可以讨论发现上面两种情况分别对应分类正确和分类错误，因此这个性质下基于梯度的学习可以很快地改正错误的<script type="math/tex">z</script>。</p>
  </li>
  <li>
    <p>此外，在软件实现中，<script type="math/tex">\mathrm{sigmoid}</script>函数可能下溢到零，这样取对数就会得到负无穷。为了避免数值问题，最好将负的对数似然写作<script type="math/tex">z</script>的函数，而不是<script type="math/tex">\hat y=\sigma(z)</script>的函数。</p>
  </li>
</ul>

<h4 id="3用于multinoulli输出分布的softmax单元">3.用于Multinoulli输出分布的softmax单元</h4>

<ul>
  <li>
    <p>任何时候当我们想要表示一个具有<script type="math/tex">n</script>个可能取值的离散型随机变量的分布时，我们都可以使用<script type="math/tex">\mathrm{softmax}</script>函数。它可以看作是<script type="math/tex">\mathrm{sigmoid}</script>函数的扩展，其中<script type="math/tex">\mathrm{sigmoid}</script>函数用来表示二值型变量的分布。</p>
  </li>
  <li>
    <p>与上一小节类似，令线性层预测为归一化的对数概率：</p>
  </li>
</ul>

<script type="math/tex; mode=display">\mathbf z=\mathbf W^T\mathbf h+\mathbf b</script>

<ul>
  <li>其中<script type="math/tex">z_i=\log \hat P(y=i\vert \mathbf x)</script>。<script type="math/tex">\mathrm{softmax}</script>函数可以对<script type="math/tex">z</script>指数化和归一化来获得需要的<script type="math/tex">\hat y</script>。最终，<script type="math/tex">\mathrm{softmax}</script>函数的形式为</li>
</ul>

<script type="math/tex; mode=display">\mathrm{softmax}(\mathbf z)_i=\frac{\exp(z_i)}{\sum_j \exp(z_j)}</script>

<ul>
  <li>
    <p>不妨设现在要求识别为第<script type="math/tex">i</script>类，在这种情况下，我们只希望第<script type="math/tex">\mathrm{softmax}(\mathbf z)_i</script>尽可能地大，其他尽可能小。</p>
  </li>
  <li>
    <p>如果使用<strong>均方误差</strong>或者<strong>平均绝对误差</strong>作为代价函数，首先会出现数值问题，其次在存在<script type="math/tex">z_j>>z_i</script>时，会输出错误的分类且代价函数对<script type="math/tex">z_i</script>的偏导也会很小甚至下溢到零，梯度算法训练效果不佳。</p>
  </li>
  <li>
    <p>为了解决上述问题，我们可以最大化<script type="math/tex">\log P(y=i;\mathbf z)=\log \mathrm{softmax}(\mathbf z)_i</script></p>
  </li>
</ul>

<script type="math/tex; mode=display">\log \mathrm{softmax}(\mathbf z)_i=z_i-\log \sum_j\exp(z_j)</script>

<ul>
  <li>可以看到，第一项输入<script type="math/tex">z_i</script>总是对代价函数有直接的贡献，不会饱和。即使存在<script type="math/tex">z_j>>z_i</script>，代价函数对<script type="math/tex">z_i</script>的偏导不会为零。而且对上式求偏导可以发现有（<script type="math/tex">F(\mathbf z)</script>指上述函数）</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial F(\mathbf z)}{\partial z_i}=1-\mathrm{softmax}(\mathbf z)_i</script>

<script type="math/tex; mode=display">\frac{\partial F(\mathbf z)}{\partial z_j}=-\mathrm{softmax}(\mathbf z)_j</script>

<ul>
  <li>不过现在虽然下溢的影响解决了，还有上溢也是很大的问题。我们可以发现，对于输出的<script type="math/tex">n</script>个概率，由于和为<script type="math/tex">1</script>，实际上只需要<script type="math/tex">n-1</script>个参数，所以实际上可以固定一个输出不变。而<script type="math/tex">\mathrm{softmax}</script>函数将所有输入加上一个常数后输出不变</li>
</ul>

<script type="math/tex; mode=display">\mathrm{softmax}(\mathbf z)=\mathrm{softmax}(\mathbf z+c)</script>

<ul>
  <li>因此可以令</li>
</ul>

<script type="math/tex; mode=display">\mathrm{softmax}(\mathbf z)=\mathrm{softmax}(\mathbf z-\max_iz_i)</script>

<ul>
  <li>
    <p>这样一来，上溢的问题也就解决了。</p>
  </li>
  <li>
    <p>此外，<script type="math/tex">\mathrm{softmax}</script>函数实际更接近<script type="math/tex">\mathrm{argmax}</script>函数而不是<script type="math/tex">\max</script>函数，它可以看做是<script type="math/tex">\mathrm{argmax}</script>函数的软化版本，而且它连续可微，而<script type="math/tex">\mathrm{argmax}</script>输出是一个one-hot向量，不是连续和可微的。<script type="math/tex">\max</script>函数的软化版本是<script type="math/tex">\mathrm{softmax}(\mathbf z)^T\mathbf z</script>。</p>
  </li>
</ul>

<h4 id="4其他的输出类型">4.其他的输出类型</h4>

<ul>
  <li></li>
</ul>

:ET