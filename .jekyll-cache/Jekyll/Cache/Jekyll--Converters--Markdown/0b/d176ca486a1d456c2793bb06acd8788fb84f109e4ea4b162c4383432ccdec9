I"l<p>（所有截图及部分文字出自《DeepLearning》中文版）</p>

<ul>
  <li>以前学习神经网络真的是浅尝辄止，根本没有好好地深入研究，学习这一部分我感觉自己有很多收获。学习过程中明显感觉自己有多菜，很多地方都是思考了很久才想通或者现在还是没搞懂，但是学完后自己还是挺欣慰的，希望我的一些个人的理解能够给读者带来一点帮助。</li>
</ul>

<h2 id="第六章-深度前馈网络">第六章 深度前馈网络</h2>

<p>关于神经网络入门有个系列视频非常推荐：</p>

<blockquote>
  <p><a href="https://www.bilibili.com/video/av15532370">深度学习之神经网络的结构</a><br />
<a href="https://www.bilibili.com/video/av16144388/?spm_id_from=333.788.videocard.0">深度学习之梯度下降法</a><br />
<a href="https://www.bilibili.com/video/av16577449/?spm_id_from=333.788.videocard.0">深度学习之反向传播算法 上</a><br />
<a href="https://www.bilibili.com/video/av16577449?p=2">深度学习之反向传播算法 下</a></p>
</blockquote>

<ul>
  <li>
    <p><strong>深度前馈网络</strong>，也叫作<strong>前馈神经网络</strong>或者<strong>多层感知机</strong></p>
  </li>
  <li>
    <p>模型被称为<strong>前向</strong>是因为输出与模型之间没有<strong>反馈</strong>连接。当包含反馈连接时，被称为<strong>循环神经网络</strong>，在第十章将会提到。</p>
  </li>
  <li>
    <p>前馈网络最后一层称为<strong>输出层</strong>。由于训练数据没有给出这些层中每一层所需的输出，因此这些层被称为<strong>隐藏层</strong>。</p>
  </li>
  <li>
    <p>可以把层想象成由许多并行操作的<strong>单元</strong>组成，每个单元表示一个向量到标量的函数。</p>
  </li>
  <li>
    <p>在神经网络发展初期，曾经就被数学家用无法学习XOR函数怼过，因为普通的神经网络是线性模型，说到底还是多个矩阵连乘，为了解决这个问题<strong>整流线性单元</strong>出现了，这个激活函数实际上就是</p>
  </li>
</ul>

<script type="math/tex; mode=display">g(z)=max\{0,z\}</script>

<h3 id="一代价函数">一、代价函数</h3>

<h4 id="1使用最大似然学习条件分布">1.使用最大似然学习条件分布</h4>

<ul>
  <li>可以使用最大似然来训练，代价函数就是负的对数似然，它与训练数据和模型分布间的交叉熵等价。该函数表示为：</li>
</ul>

<script type="math/tex; mode=display">J(\mathbf \theta)=-\mathbb E_{\mathbf{x,y}\sim\hat p_{data}}\log p_{model}(\mathbf y\vert \mathbf x)</script>

<ul>
  <li>上述方程展开后通常会有一些项不依赖于模型的参数。比如若有</li>
</ul>

<script type="math/tex; mode=display">p_{model}(\mathbf y\vert\mathbf x)=\mathcal{N}(\mathbf y\vert f(\mathbf x;\mathbf \theta),\mathbf I)</script>

<ul>
  <li>那么我们就重新得到了均方误差代价（高斯分布函数取对数之后是均方误差）</li>
</ul>

<script type="math/tex; mode=display">J(\mathbf \theta)=\frac{1}{2}\mathbb E_{\mathbf{x,y}\sim \hat p_{data}}\parallel\mathbf y-f(\mathbf x;\mathbf \theta)\parallel^2+const</script>

<ul>
  <li>使用最大似然导出代价函数的一个优势就是，减轻了为模型设计代价函数的负担，因为只要明确一个模型<script type="math/tex">p(\mathbf y\vert\mathbf x)</script>就自动地确定了一个代价函数<script type="math/tex">\log p(\mathbf y\vert\mathbf x)</script>。</li>
</ul>

<h4 id="2学习条件统计量">2.学习条件统计量</h4>

<ul>
  <li>
    <p>有时我们并不想学习一个完整的概率分布<script type="math/tex">p(\mathbf y\vert \mathbf x)</script>，而仅仅是想学习在给定<script type="math/tex">\mathbf x</script>时<script type="math/tex">\mathbf y</script>的某个条件统计量。</p>
  </li>
  <li>
    <p>例如，有一个预测器<script type="math/tex">f(\mathbf x;\mathbf \theta)</script>，我们想用它来预测<script type="math/tex">\mathbf y</script>的均值。</p>
  </li>
  <li>
    <p>我们可以认为神经网络能够表示一大类函数中任何一个函数<script type="math/tex">f</script>，可以把代价函数看作是一个<strong>泛函</strong>，泛函是函数到实数的映射，也就是要学习一个函数使得代价最小。</p>
  </li>
  <li>
    <p>可以使用<strong>变分法</strong>导出第一个结果是解优化问题（这里不知道变分法是啥。。在十九章会学习）</p>
  </li>
</ul>

<script type="math/tex; mode=display">f^*=\underset{f}{\mathrm{argmin}}\mathbb E_{\mathbf{x,y}\sim p_{data}}\vert\vert\mathbf y-f(\mathbf x)\vert\vert^2</script>

<ul>
  <li>得到</li>
</ul>

<script type="math/tex; mode=display">f^*(\mathbf x)=\mathbb E_{\mathbf y\sim p_{data}(\mathbf y\vert\mathbf x)}[\mathbf y]</script>

<ul>
  <li>
    <p>要求这个函数处在我们要优化的类里。</p>
  </li>
  <li>
    <p>不同代价函数给出不同的统计量。第二个会用变分法得到的结果是</p>
  </li>
</ul>

<script type="math/tex; mode=display">f^*=\underset{f}{\mathrm{argmin}}\mathbb E_{\mathbf{x,y}\sim p_{data}}\vert\vert\mathbf y-f(\mathbf x)\vert\vert_1</script>

<ul>
  <li>
    <p>将得到一个函数可以对每个<script type="math/tex">\mathbf x</script>预测<script type="math/tex">\mathbf y</script>取值的中位数，只要这个函数在我们要优化的函数族里。这个代价函数通常被称为<strong>平均绝对误差</strong></p>
  </li>
  <li>
    <p><strong>均方误差</strong>和<strong>平均绝对误差</strong>在使用基于梯度的优化方法时往往成效不佳。一些饱和的输出单元结合这些代价函数会产生很小的梯度。这是<strong>交叉熵</strong>代价函数比他们更受欢迎的原因之一，即使是在没有必要估计整个<script type="math/tex">p(\mathbf y\vert\mathbf x)</script>分布时。</p>
  </li>
</ul>

<h3 id="二输出单元">二、输出单元</h3>

<ul>
  <li>
    <p>代价函数的选择与输出单元的选择紧密相关。大多数时候，我们简单地使用数据分布和模型分布间的交叉熵。选择如何表示输出决定了交叉熵函数的形式。</p>
  </li>
  <li>
    <p>任何可用作输出的神经网络单元，也可以被用作隐藏单元。</p>
  </li>
  <li>
    <p>本节中，假设前馈网络提供了一组定义为<script type="math/tex">\mathbf h=f(\mathbf x;\mathbf \theta)</script>的隐藏特征。输出层的作用是随后对这些特征进行额外的变换来完成整个网络必须完成的任务。</p>
  </li>
</ul>

<h4 id="1用于高斯输出分布的线性单元">1.用于高斯输出分布的线性单元</h4>

<ul>
  <li>
    <p>一种简单的输出单元是基于仿射变换的输出单元，仿射变换不具有非线性，这些单元往往被称为线性单元。</p>
  </li>
  <li>
    <p>给定特征<script type="math/tex">\mathbf h</script>，线性输出层产生一个向量<script type="math/tex">\hat{\mathbf y}=\mathbf W^T\mathbf h+b</script></p>
  </li>
  <li>
    <p>线性输出层经常被用来产生条件高斯分布的均值：</p>
  </li>
</ul>

<script type="math/tex; mode=display">p(\mathbf y\vert\mathbf x)=\mathcal N(\mathbf y;\hat{\mathbf y},\mathbf I)</script>

<ul>
  <li>
    <p>最大化其对数似然此时等价于最小化均方误差。</p>
  </li>
  <li>
    <p>因为线性模型不会饱和，所以易于采用基于梯度的优化算法，甚至可以使用其他多种优化算法。</p>
  </li>
</ul>

<h4 id="2用于bernoulli输出分布的sigmoid单元">2.用于Bernoulli输出分布的sigmoid单元</h4>

<ul>
  <li>
    <p>许多任务需要预测二值型变量<script type="math/tex">y</script>的值。具有两个类的分类问题可以归结为这种形式。</p>
  </li>
  <li>
    <p>神经网络只需要预测<script type="math/tex">P(y=1\vert \mathbf x)</script>即可。为了使这个数是有效的概率，它必须处在区间<script type="math/tex">[0,1]</script>中。</p>
  </li>
  <li>
    <p>如果使用线性单元对<script type="math/tex">0,1</script>分别取最大值和最小值，则梯度下降法求导时会变得更复杂。而且在区间外时梯度直接变成<script type="math/tex">0</script>，这样就无法进行学习了。</p>
  </li>
  <li>
    <p>需要一种新方法来保证无论何时模型给出了错误的答案时，总能有一个较大的梯度。这种方法是基于使用<script type="math/tex">\mathrm{sigmoid}</script>输出单元结合最大似然来实现的。</p>
  </li>
  <li>
    <p><script type="math/tex">\mathrm{sigmoid}</script>输出单元定义为</p>
  </li>
</ul>

<script type="math/tex; mode=display">\hat y=\sigma(\mathbf w^T\mathbf h+b)</script>

<ul>
  <li>这里<script type="math/tex">\sigma</script>是第三章中介绍的<script type="math/tex">logistic\;\mathrm{sigmoid}</script>函数</li>
</ul>

<script type="math/tex; mode=display">\sigma(x)=\frac{1}{1+\exp(-x)}</script>

<ul>
  <li>
    <p>我们可以将<script type="math/tex">\mathrm{sigmoid}</script>输出单元看成线性层<script type="math/tex">z=\mathbf w^T\mathbf h+b</script>和<script type="math/tex">\mathrm{sigmoid}</script>激活函数将<script type="math/tex">z</script>转化成概率。</p>
  </li>
  <li>
    <p>我们暂时忽略对于<script type="math/tex">\mathbf x</script>的依赖性，只讨论<script type="math/tex">z</script>的值来定义<script type="math/tex">y</script>的概率分布。</p>
  </li>
  <li>
    <p>首先构造一个非归一化（和不为<script type="math/tex">1</script>）的概率分布<script type="math/tex">\tilde P(y)</script>，假定其对数概率为</p>
  </li>
</ul>

<script type="math/tex; mode=display">\log \tilde P(y)=yz</script>

<ul>
  <li>取指数得到非归一化概率</li>
</ul>

<script type="math/tex; mode=display">\tilde P(y)=\exp(yz)</script>

<ul>
  <li>上面两个公式个人理解是赋予网络输出<script type="math/tex">z</script>一个合理的意义，可以观察到非归一化概率<script type="math/tex">\tilde P(y=0)=1,\tilde P(y=1)=\exp(z)</script>，也就是通过<script type="math/tex">z</script>的大小作为二值分布概率的相对差别。归一化可得</li>
</ul>

<script type="math/tex; mode=display">P(y)=\frac{\tilde P(y)}{\sum_{y'=0}^1\tilde P(y=y')}=\frac{\exp(yz)}{\sum_{y'=0}^{1}\exp(y'z)}</script>

<script type="math/tex; mode=display">P(y)=\sigma((2y-1)z)</script>

<ul>
  <li>
    <p>上面第二个公式乍一看有点跳跃，实际上把<script type="math/tex">y=0,1</script>分别代入可以发现是正确的。</p>
  </li>
  <li>
    <p>用于定义这种二值型变量分布的变量<script type="math/tex">z</script>被称为<strong>分对数</strong>。</p>
  </li>
  <li>
    <p>这样很自然地可以使用最大似然代价函数</p>
  </li>
</ul>

<script type="math/tex; mode=display">% <![CDATA[
\begin{align}
J(\mathbf \theta)=&-\log P(y\vert \mathbf x)\\
=&-\log \sigma((2y-1)z)\\
=&\zeta((1-2y)z)
\end{align} %]]></script>

<ul>
  <li>
    <p><script type="math/tex">\zeta</script>就是<script type="math/tex">\mathrm{softplus}</script>函数，<script type="math/tex">\zeta(x)=\log(1+\exp(x))</script>，可以看做是平滑版的<script type="math/tex">\mathrm{Relu}</script>函数。而且由于函数类似于<script type="math/tex">\mathrm{Relu}</script>，在<script type="math/tex">% <![CDATA[
x<0 %]]></script>时梯度会快速衰减到很小，在<script type="math/tex">x>0</script>时梯度不会收缩。可以讨论发现上面两种情况分别对应分类正确和分类错误，因此这个性质下基于梯度的学习可以很快地改正错误的<script type="math/tex">z</script>。</p>
  </li>
  <li>
    <p>此外，在软件实现中，<script type="math/tex">\mathrm{sigmoid}</script>函数可能下溢到零，这样取对数就会得到负无穷。为了避免数值问题，最好将负的对数似然写作<script type="math/tex">z</script>的函数，而不是<script type="math/tex">\hat y=\sigma(z)</script>的函数。</p>
  </li>
</ul>

<h4 id="3用于multinoulli输出分布的softmax单元">3.用于Multinoulli输出分布的softmax单元</h4>

<ul>
  <li>
    <p>任何时候当我们想要表示一个具有<script type="math/tex">n</script>个可能取值的离散型随机变量的分布时，我们都可以使用<script type="math/tex">\mathrm{softmax}</script>函数。它可以看作是<script type="math/tex">\mathrm{sigmoid}</script>函数的扩展，其中<script type="math/tex">\mathrm{sigmoid}</script>函数用来表示二值型变量的分布。</p>
  </li>
  <li>
    <p>与上一小节类似，令线性层预测为归一化的对数概率：</p>
  </li>
</ul>

<script type="math/tex; mode=display">\mathbf z=\mathbf W^T\mathbf h+\mathbf b</script>

<ul>
  <li>其中<script type="math/tex">z_i=\log \hat P(y=i\vert \mathbf x)</script>。<script type="math/tex">\mathrm{softmax}</script>函数可以对<script type="math/tex">z</script>指数化和归一化来获得需要的<script type="math/tex">\hat y</script>。最终，<script type="math/tex">\mathrm{softmax}</script>函数的形式为</li>
</ul>

<script type="math/tex; mode=display">\mathrm{softmax}(\mathbf z)_i=\frac{\exp(z_i)}{\sum_j \exp(z_j)}</script>

<ul>
  <li>
    <p>不妨设现在要求识别为第<script type="math/tex">i</script>类，在这种情况下，我们只希望第<script type="math/tex">\mathrm{softmax}(\mathbf z)_i</script>尽可能地大，其他尽可能小。</p>
  </li>
  <li>
    <p>如果使用<strong>均方误差</strong>或者<strong>平均绝对误差</strong>作为代价函数，首先会出现数值问题，其次在存在<script type="math/tex">z_j>>z_i</script>时，会输出错误的分类且代价函数对<script type="math/tex">z_i</script>的偏导也会很小甚至下溢到零，梯度算法训练效果不佳。</p>
  </li>
  <li>
    <p>为了解决上述问题，我们可以最大化<script type="math/tex">\log P(y=i;\mathbf z)=\log \mathrm{softmax}(\mathbf z)_i</script></p>
  </li>
</ul>

<script type="math/tex; mode=display">\log \mathrm{softmax}(\mathbf z)_i=z_i-\log \sum_j\exp(z_j)</script>

<ul>
  <li>可以看到，第一项输入<script type="math/tex">z_i</script>总是对代价函数有直接的贡献，不会饱和。即使存在<script type="math/tex">z_j>>z_i</script>，代价函数对<script type="math/tex">z_i</script>的偏导不会为零。而且对上式求偏导可以发现有（<script type="math/tex">F(\mathbf z)</script>指上述函数）</li>
</ul>

<script type="math/tex; mode=display">\frac{\partial F(\mathbf z)}{\partial z_i}=1-\mathrm{softmax}(\mathbf z)_i</script>

<script type="math/tex; mode=display">\frac{\partial F(\mathbf z)}{\partial z_j}=-\mathrm{softmax}(\mathbf z)_j</script>

<ul>
  <li>不过现在虽然下溢的影响解决了，还有上溢也是很大的问题。我们可以发现，对于输出的<script type="math/tex">n</script>个概率，由于和为<script type="math/tex">1</script>，实际上只需要<script type="math/tex">n-1</script>个参数，所以实际上可以固定一个输出不变。而<script type="math/tex">\mathrm{softmax}</script>函数将所有输入加上一个常数后输出不变</li>
</ul>

<script type="math/tex; mode=display">\mathrm{softmax}(\mathbf z)=\mathrm{softmax}(\mathbf z+c)</script>

<ul>
  <li>因此可以令</li>
</ul>

<script type="math/tex; mode=display">\mathrm{softmax}(\mathbf z)=\mathrm{softmax}(\mathbf z-\max_iz_i)</script>

<ul>
  <li>
    <p>这样一来，上溢的问题也就解决了。</p>
  </li>
  <li>
    <p>此外，<script type="math/tex">\mathrm{softmax}</script>函数实际更接近<script type="math/tex">\mathrm{argmax}</script>函数而不是<script type="math/tex">\max</script>函数，它可以看做是<script type="math/tex">\mathrm{argmax}</script>函数的软化版本，而且它连续可微，而<script type="math/tex">\mathrm{argmax}</script>输出是一个one-hot向量，不是连续和可微的。<script type="math/tex">\max</script>函数的软化版本是<script type="math/tex">\mathrm{softmax}(\mathbf z)^T\mathbf z</script>。</p>
  </li>
</ul>

<h4 id="4其他的输出类型">4.其他的输出类型</h4>

<ul>
  <li>
    <p>之前描述的线性、sigmoid 和 softmax 输出单元是最常见的。神经网络可以推广到我们希望的几乎任何种类的输出层。最大似然原则给如何为几乎任何种类的输出层设计一个好的代价函数提供了指导。</p>
  </li>
  <li>
    <p>一般的，如果我们定义了一个条件分布<script type="math/tex">p(\mathbf y\vert \mathbf x;\mathbf \theta)</script>，最大似然原则建议我们使用<script type="math/tex">-\log p(\mathbf y\vert \mathbf x;\mathbf \theta)</script>作为代价函数。</p>
  </li>
  <li>
    <p>一般来说，我们可以认为神经网络表示函数<script type="math/tex">f(\mathbf x;\mathbf \theta)</script>。这个函数的输出不是对<script type="math/tex">\mathbf y</script>值的直接预测。相反，<script type="math/tex">f(\mathbf x;\mathbf \theta) =\mathbf \omega</script>提供了<script type="math/tex">\mathbf y</script>分布的参数。我们的损失函数就可以表示成<script type="math/tex">-\log p(\mathbf y;\mathbf \omega(\mathbf x))</script>。</p>
  </li>
  <li>
    <p>我们经常想要执行多峰回归，即预测条件分布<script type="math/tex">p(\mathbf y\vert\mathbf x)</script>的实值，该条件分布对于相同的<script type="math/tex">\mathbf x</script>值在<script type="math/tex">\mathbf y</script>空间中有多个不同的峰值。在这种情况下，<strong>高斯混合</strong>是输出的自然表示。将高斯混合作为其输出的神经网络通常被称为<script type="math/tex">混合密度网络</script>。具有<script type="math/tex">n</script>个分量的高斯混合输出由下面的条件分布定义：</p>
  </li>
</ul>

<script type="math/tex; mode=display">p(\mathbf y\vert\mathbf x)=\sum_{i=1}^np(c=i\vert \mathbf x)\mathcal N(\mathbf y;\mathbf \mu^{(i)}(\mathbf x),\Sigma^{(i)}(\mathbf x))</script>

<ul>
  <li>
    <p>神经网络必须有三个输出：定义<script type="math/tex">p(c=i\vert \mathbf x)</script>的向量，对所有的<script type="math/tex">i</script>给出<script type="math/tex">\mathbf \mu^{(i)}(\mathbf x)</script>的矩阵，以及对所有的<script type="math/tex">i</script>给出<script type="math/tex">\Sigma^{(i)}(\mathbf x)</script>的张量。</p>
  </li>
  <li>
    <p>高斯混合输出在<strong>语音生成模型</strong>和<strong>物理运动</strong>中特别有效。混合密度策略为网络提供了一种方法来表示多种输出模式，并且控制输出的方差，这对于在这些实数域中获得高质量的结果是至关重要的。混合密度网络的一个实例如下图所示。</p>
  </li>
</ul>

<p><img src="https://raw.githubusercontent.com/chongjg/chongjg.github.io/master/img/deeplearning/deeplearning-35.png" alt="" /></p>

<h3 id="三隐藏单元">三、隐藏单元</h3>

<h4 id="1整流线性单元及其扩展">1.整流线性单元及其扩展</h4>

<ul>
  <li>
    <p>整流线性单元使用激活函数<script type="math/tex">g(z)=\max\{0,z\}</script></p>
  </li>
  <li>
    <p>由于在<script type="math/tex">z=0</script>时导数不存在，在软件实现中往往不视作不存在，而是直接使用左导数<script type="math/tex">0</script>或者右导数<script type="math/tex">1</script>。</p>
  </li>
  <li>
    <p>整流线性单元通常作用于仿射变换之上：</p>
  </li>
</ul>

<script type="math/tex; mode=display">\mathbf h=g(\mathbf W^T\mathbf x+\mathbf b)</script>

<ul>
  <li>
    <p>当初始化仿射变换的参数时，可以将<script type="math/tex">\mathbf b</script>的所有元素设置成一个小的正值，例如<script type="math/tex">0.1</script>。这使得初始时大多数整流线性单元处于激活状态，并且允许导数通过。</p>
  </li>
  <li>
    <p>整流线性单元的一个缺陷是它们不能通过基于梯度的方法学习那些使它们激活为零的样本。整流线性单元的各种扩展保证了它们能够在各个位置上都接收到梯度。</p>
  </li>
  <li>
    <p>整流线性单元的三个扩展基于当<script type="math/tex">% <![CDATA[
z_i<0 %]]></script>时使用一个非零的斜率<script type="math/tex">\alpha_i</script>：<script type="math/tex">h_i=g(\mathbf z,\mathbf \alpha)_i=\max(0,z_i)+\alpha_i \min(0,z_i)</script></p>
  </li>
  <li>
    <p><strong>绝对值整流</strong>固定<script type="math/tex">\alpha_i=-1</script>来得到<script type="math/tex">g(z)=\vert z\vert</script>。它用于图像中的对象识别，其中在寻找照明极性反转下不变的特征是有意义的。</p>
  </li>
  <li>
    <p><strong>渗漏整流线性单元</strong>将<script type="math/tex">\alpha_i</script>固定成一个类似<script type="math/tex">0.01</script>的小值。</p>
  </li>
  <li>
    <p><strong>参数化整流线性单元</strong>或者<strong>PReLU</strong>将<script type="math/tex">\alpha_i</script>作为学习的参数。</p>
  </li>
  <li>
    <p><strong>maxout单元</strong>进一步扩展了线性整流，<script type="math/tex">\mathrm{maxout}</script>单元将<script type="math/tex">\mathbf z</script>划分为每组具有<script type="math/tex">k</script>个值得组，而不是使用作用于每个元素的函数<script type="math/tex">g(z)</script>。每个<script type="math/tex">\mathrm{maxout}</script>单元则输出每组中的最大元素：</p>
  </li>
</ul>

<script type="math/tex; mode=display">g(\mathbf z)_i=\max_{j\in\mathbb G^{(i)}}z_j</script>

<ul>
  <li>
    <p>这里<script type="math/tex">\mathbb G^{(i)}</script>是组<script type="math/tex">i</script>的输入索引集<script type="math/tex">\{(i-1)k+1,...,ik\}</script>。这提供了一种方法来学习对输入<script type="math/tex">x</script>空间中多个方向响应的分段线性函数。</p>
  </li>
  <li>
    <p>整流线性单元和它们的这些扩展都是基于一个原则，那就是如果它们的行为更接近线性，那么模型更容易优化。</p>
  </li>
</ul>

<h4 id="2logistic-sigmoid与双曲正切函数">2.logistic sigmoid与双曲正切函数</h4>

<ul>
  <li>在引如整流线性单元之前，大多数神经网络使用<script type="math/tex">\mathrm{logistic\;sigmoid}</script>激活函数</li>
</ul>

<script type="math/tex; mode=display">g(z)=\sigma(z)</script>

<ul>
  <li>或者是双曲正切激活函数</li>
</ul>

<script type="math/tex; mode=display">g(z)=\tanh(z)</script>

<ul>
  <li>
    <p>这两个激活函数紧密相关，有<script type="math/tex">\tanh(z)=2\sigma(2z)-1</script></p>
  </li>
  <li>
    <p>我们之前已经有对<script type="math/tex">\mathrm{sigmoid}</script>函数进行过一些分析，它在大部分定义域内都是饱和的，也就是梯度接近零，它的广泛饱和性会使得基于梯度的学习变得非常困难。因为这个原因，现在不鼓励将他们用作前馈网络中的隐藏单元。当使用一个合适的代价函数来抵消<script type="math/tex">\mathrm{sigmoid}</script>的饱和性时，他们作为输出单元可以与基于梯度的学习相兼容。</p>
  </li>
  <li>
    <p>当必须要使用<script type="math/tex">\mathrm{sigmoid}</script>激活函数时，双曲正切激活函数通常要比<script type="math/tex">\mathrm{logistic\;sigmoid}</script>函数表现更好。在<script type="math/tex">\tanh(0)=0</script>而<script type="math/tex">\sigma(0)=\frac{1}{2}</script>的意义上，它更像恒等函数<script type="math/tex">f(x)=x</script>（这里中文书翻译成单位函数应该是搞错了，英文是<script type="math/tex">\mathrm{identity\;function}</script>）。因为<script type="math/tex">\tanh</script>在<script type="math/tex">0</script>附近与恒等函数类似，训练深层神经网络<script type="math/tex">\hat y=\mathbf w^T\tanh(\mathbf U^T\tanh(\mathbf V^T\mathbf x))</script>类似于训练一个线性模型<script type="math/tex">\hat y=\mathbf w^T\mathbf U^T\mathbf V^T\mathbf x</script>，只要网络的激活能够被保持地很小。这使得训练<script type="math/tex">\tanh</script>网络更加容易。</p>
  </li>
  <li>
    <p><script type="math/tex">\mathrm{sigmoid}</script>激活函数在除了前馈网络之外的情景中更为常见。循环网络、许多概率模型以及一些自编码器有一些额外的要求使得它们不能使用分段线性激活函数，并且使得<script type="math/tex">\mathrm{sigmoid}</script>单元更具有吸引力，尽管它存在饱和性的问题。</p>
  </li>
</ul>

<h4 id="3其他隐藏单元">3.其他隐藏单元</h4>

<ul>
  <li>
    <p>也存在许多其他种类的隐藏单元，但它们并不常用。</p>
  </li>
  <li>
    <p>比如，作者在<script type="math/tex">/mathrm{MNIST}</script>数据集上使用<script type="math/tex">\mathbf h=\cos(\mathbf{Wx}+\mathbf b)</script>测试了一个前馈网络，并获得了小于<script type="math/tex">1%</script>的误差率，这可以与更为传统的激活函数获得的结果相媲美。</p>
  </li>
  <li>
    <p>列出文献中出现的所有隐藏单元类型是不切实际的。我们只对一些特别有用和独特的类型进行强调。</p>
  </li>
  <li>
    <p>其中一种是完全没有激活函数。也可以认为是使用<script type="math/tex">g(z)=z</script>作为激活函数。考虑具有<script type="math/tex">n</script>个输入和<script type="math/tex">p</script>个输出的神经网络层<script type="math/tex">\mathbf h=g(\mathbf W^T\mathbf x+\mathbf b)</script>。我们可以用<script type="math/tex">\mathbf h=g(\mathbf V^T\mathbf U^T\mathbf x+\mathbf b)</script>来代替它，如果<script type="math/tex">\mathbf U</script>产生了<script type="math/tex">q</script>个输出，那么<script type="math/tex">\mathbf U</script>和<script type="math/tex">\mathbf V</script>一起仅包含<script type="math/tex">(n+p)q</script>个参数，而<script type="math/tex">\mathbf W</script>包含<script type="math/tex">np</script>个参数，如果<script type="math/tex">q</script>很小，这可以在很大程度上节省参数。这是以将线性变换约束为低秩的代价来实现的，但这些低秩关系往往是足够的。线性隐藏单元因此提供了一种减少网络中参数数量的有效方法。</p>
  </li>
  <li>
    <p><script type="math/tex">\mathrm{softmax}</script>单元是另一种经常用作输入的单元，但有时也用作隐藏单元，可以作为一种开关。这些类型的隐藏单元通常仅用于明确地学习操作内存的高级结构中，将在第十章中描述。</p>
  </li>
  <li>
    <p>其他一些常见的隐藏单元类型包括：</p>
  </li>
  <li>
    <p><strong>径向基函数</strong>：<script type="math/tex">h_i=\exp\big(-\frac{1}{\sigma_i^2}\parallel\mathbf W_{:,i}-\mathbf x\parallel^2\big)</script>。这个函数在<script type="math/tex">\mathbf x</script>接近模板<script type="math/tex">\mathbf W_{:,i}</script>时更加活跃。因为它对大部分<script type="math/tex">\mathbf x</script>都饱和到<script type="math/tex">0</script>，因此很难优化。</p>
  </li>
  <li>
    <p><script type="math/tex">\mathbf{\mathrm{softplus}}</script><strong>函数</strong>：<script type="math/tex">g(a)=\zeta(a)=\log(1+\exp(a))</script>。这是整流线性单元的平滑版本。虽然它看起来比整流线性单元更好，但实际测试中效果不如整流线性单元。</p>

  </li>
</ul>
:ET